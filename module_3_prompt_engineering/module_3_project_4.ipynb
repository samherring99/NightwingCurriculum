{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 Project 4: DSPy\n",
    "- Implement a basic example using [DSPy](https://dspy-docs.vercel.app/) to get comfortable with how it works\n",
    "- Note: this notebook should be run using a GPU and with `python3.9` - it also requires [TGI](https://github.com/huggingface/text-generation-inference) from HF to be running locally (alternatively, you can use your HuggingFace API key by uncommenting below)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dspy-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: IMPORTS\n",
    "- We need to import `dspy` and our dataset + metric evals for [GSM8K](https://huggingface.co/datasets/gsm8k) (Grade School Math 8k)\n",
    "- We also need to import our optimizer and evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy.datasets.gsm8k import GSM8K, gsm8k_metric\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "from dspy.evaluate import Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: LOAD THE MODEL\n",
    "- We load the model in this step\n",
    "- We will be using [TheBloke's Mistral-7B-Instruct-v0.2-GPTQ](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GPTQ) model for this\n",
    "- Configure dspy to use the model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two options for using Huggingface, though the non-TGI version has some known bugs generating multiple line answers\n",
    "llama = dspy.HFClientTGI(model=\"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\", port=8080, url=\"http://localhost\")\n",
    "#llama = dspy.HFModel(model = 'TheBloke/Mistral-7B-Instruct-v0.2-GPTQ')\n",
    "\n",
    "dspy.settings.configure(lm=llama)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: GSM8K DATASET\n",
    "- Grade School Math (8K) is a dataset of math problems and answers \n",
    "- Answers include reasoning traces and calulations to help guide the model towards computation\n",
    "- We only take the first 10 examples from each set for this demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our dataset and split into train and validation\n",
    "gsm8k = GSM8K()\n",
    "gsm8k_trainset, gsm8k_devset = gsm8k.train[:10], gsm8k.dev[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: SIMPLE DSPY MODULE\n",
    "- Next, we create our simple [dspy.Module](https://dspy-docs.vercel.app/api/category/modules)\n",
    "- It consists of one `ChainOfThought` layer with the signature: question -> answer\n",
    "- The signature above denotes that there is a 'question' being asked, and that the model needs to respond with the 'answer' \n",
    "- Providing the 'question' from GSM8k leads to the model attempting to answer it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DSPy module for CoT - we have 1 signature that takes in a question and returns an answer\n",
    "class CoT(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.ChainOfThought(\"question -> answer\")\n",
    "    \n",
    "    # Forward pass prints the answer\n",
    "    def forward(self, question):\n",
    "        resp = self.prog(question=question)\n",
    "        print(resp)\n",
    "        return resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: COMPILE\n",
    "- Now we can compile our module created above into a dspy program\n",
    "- Using out config of max 4 bootstrapped demos per run, and 4 maximum labeled demos\n",
    "- We use `BootstrapFewShotWithRandomSearch` optimizer with the `gsm8k_metric` validation metric to measure accuracy over the dataset questions\n",
    "- Lastly, we can use our optimizer to compile our module with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(max_bootstrapped_demos=4, max_labeled_demos=4)\n",
    "\n",
    "# Optimizer is BootstrapFewShotWithRandomSearch as this is best for this sample size\n",
    "teleprompter = BootstrapFewShotWithRandomSearch(metric=gsm8k_metric, **config)\n",
    "\n",
    "# Compile our module with our data\n",
    "optimized_cot = teleprompter.compile(CoT(), trainset=gsm8k_trainset, valset=gsm8k_devset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: EVALUATE\n",
    "- The final step here is to evaluate over our data\n",
    "- We set 1 thread here, and run the evaluation\n",
    "- Our optimizer will loop over a set # of seeds, re-iterating over the data and trying to tune for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up evaluation step\n",
    "evaluate = Evaluate(devset=gsm8k_devset, metric=gsm8k_metric, num_threads=1, display_progress=True, display_table=0)\n",
    "\n",
    "# Evaluate and display\n",
    "evaluate(optimized_cot)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
