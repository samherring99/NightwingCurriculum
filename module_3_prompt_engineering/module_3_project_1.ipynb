{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 Project 1: Prompt Engineering\n",
    "- Create a basic prompt engineering loop with different variations of the same prompt.\n",
    "- Run the loop over the examples and evalualte the results yourself (automated eval comes later)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: IMPORTS\n",
    "- Import [LangChain](https://www.langchain.com/) for `llama-cpp-python` wrapper (makes our life simpler for now)\n",
    "- Import regex for prompt extraction and time for rate limiting (gives our CPU some time to cool down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: LOAD THE MODEL\n",
    "- Load the model from your file path (`.gguf` format)\n",
    "- The below parameters are used with Apple Silicon to run on your CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GPU layers and batch size\n",
    "n_gpu_layers = 1\n",
    "n_batch = 4096  \n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# Instantiate the model\n",
    "language_model = LlamaCpp(\n",
    "    model_path=\"/path/to/your/model.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=4096,\n",
    "    f16_kv=True,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=False,\n",
    "    echo=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: LOAD THE PROMPTS\n",
    "- We want to load our prompt file, where each prompt is delimited with \"===\"\n",
    "- Read in the file and split out the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path for prompt list separated by '==='\n",
    "file_path = \"prompts_codegen.txt\"\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "text = read_file(file_path)\n",
    "\n",
    "# Read the prompts into a list\n",
    "results = re.findall(r'===(.*?)===', text, re.DOTALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: ITERATE\n",
    "- Given our list of prompts, run each one every 5 seconds\n",
    "- Results are displayed through LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the prompts, and prompt the LLM with each one\n",
    "for result in results:\n",
    "    print(\"Prompt: \\n\" + result.strip())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    result = language_model(result.strip())\n",
    "    time.sleep(5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
