{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 Project 5: Automated Evaluation with DSPy + CoT\n",
    "We want to tie everything in this module together into a single project\n",
    "Pick a new domain, use DSPy to build out the modules to specify the problem space\n",
    "We will be prototyping a simple prompt+evaluation loop for a very basic Software Engineering agent using Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install dspy-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: IMPORTS\n",
    "- We need to import [DSPy](https://dspy-docs.vercel.app/docs/building-blocks/solving_your_task) and related pieces for optimization and evaluation\n",
    "- We also need a Dataloader for our dataset and system imports like `math` and `re`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy.datasets import DataLoader\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "import math\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: LOAD THE MODEL\n",
    "- We now load our LLM, which is [TheBloke's Mistral-7B-Instruct-v0.2-GPTQ](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GPTQ) running locally on a [TGI server](https://github.com/huggingface/text-generation-inference)\n",
    "- We want to configure dspy to use the settings from this model's config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = dspy.HFClientTGI(model=\"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\", port=8080, url=\"http://localhost\")\n",
    "\n",
    "dspy.settings.configure(lm=llama)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: LOAD THE DATASET\n",
    "- We will be using [CodeAlpaca_20K](https://huggingface.co/datasets/HuggingFaceH4/CodeAlpaca_20K) for our dataset\n",
    "- This data consists of 'prompt' sections in English, and a code section marked as 'completion' for each example\n",
    "- We will denote that the 'prompt' is the desired input here in each dataset\n",
    "- We will take the first 100 examples from the train and test splits respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD = re.compile(r\"\\w+\")\n",
    "\n",
    "dl = DataLoader()\n",
    "\n",
    "code_alpaca = dl.from_huggingface(\"HuggingFaceH4/CodeAlpaca_20K\")\n",
    "\n",
    "train_dataset = [x.with_inputs('prompt') for x in code_alpaca['train']][:100]\n",
    "test_dataset = [x.with_inputs('prompt') for x in code_alpaca['test']][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: SIGNATURES\n",
    "- Here we define [Signatures](https://dspy-docs.vercel.app/docs/building-blocks/signatures) for our program\n",
    "- Instead of doing shorthand like \"question -> answer\", we can make Signatures with descriptions\n",
    "- These descriptions are used as inputs with the prompts for each step to the model\n",
    "- This allows us to steer the model towards each task required at each step of the program\n",
    "- The steps we want our 'coder' to take are: \n",
    "\n",
    "```\n",
    "1. Problem Analysis\n",
    "2. Solution Generation\n",
    "3. Generate Candidate Solutions\n",
    "4. Rank Generated Solutions\n",
    "5. Generate Test for Top Solution\n",
    "6. Validate Code/Test Pair\n",
    "7. Iterate and Improve\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Problem(dspy.Signature):\n",
    "    prompt = dspy.InputField(desc='The prompt section of the given Example')\n",
    "    analysis = dspy.OutputField(desc=\"Simple instructions on how to solve this problem in code\")\n",
    "\n",
    "class Solution(dspy.Signature):\n",
    "    analysis = dspy.InputField()\n",
    "    code_solution = dspy.OutputField(desc=\"A code solution in a single line with no newlines enclosed in backticks answering the input\")\n",
    "\n",
    "class SolutionWithAttempt(dspy.Signature):\n",
    "    analysis = dspy.InputField()\n",
    "    previous_attempt = dspy.InputField()\n",
    "    code_solution = dspy.OutputField(desc=\"A code solution in a single line with no newlines enclosed in backticks answering the input that is different than the provided attempt\")\n",
    "\n",
    "class Ranker(dspy.Signature):\n",
    "    code_solutions = dspy.InputField()\n",
    "    top_ranked_code_solution = dspy.OutputField(desc=\"Sort the given code solutions by clarity and syntax and return the top one no other text\")\n",
    "\n",
    "class Test(dspy.Signature):\n",
    "    top_solution = dspy.InputField()\n",
    "    test = dspy.OutputField(desc=\"A test for the given code solution in a single line with no newlines enclosed in backticks\")\n",
    "\n",
    "class Judge(dspy.Signature):\n",
    "    code = dspy.InputField()\n",
    "    test = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"True/False whether the code solution and accompanying test are syntactically correct and if the test will pass when ran no other text\")\n",
    "\n",
    "class Improve(dspy.Signature):\n",
    "    code = dspy.InputField()\n",
    "    test = dspy.InputField()\n",
    "    top_ranked_code_solution = dspy.OutputField(desc=\"Improve the given code solution according to the provided test and return the improved code solution no other text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: MODULE\n",
    "- Now that we've created our signatures, we can build our [Module](https://dspy-docs.vercel.app/docs/building-blocks/modules)\n",
    "- We want to walk through the steps listed above, with the analysis step set as CoT to capture rationale\n",
    "- We define our initialization to build our internal modules with [Predict and ChainOfThought](https://dspy-docs.vercel.app/docs/building-blocks/modules#how-do-i-use-a-built-in-module-like-dspypredict-or-dspychainofthought)\n",
    "- We define our `forward` step to walk through our modules and pass inputs in accordingly\n",
    "- We generate 4 candidate solutions initially, and give the model 5 attempts to improve on the best solution\n",
    "- The 'criteria' for validation here is simply just another LM call to return `True/False` - since this is a prototype and demonstration, this is okay, but in production we would need better evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NightwingCoder(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.problem_reflector = dspy.ChainOfThought(Problem)\n",
    "        self.solution_generator = dspy.Predict(Solution)\n",
    "        self.solution_improver = dspy.Predict(SolutionWithAttempt)\n",
    "        self.rank_solutions = dspy.Predict(Ranker)\n",
    "        self.generate_test = dspy.Predict(Test)\n",
    "        self.evaluate_solution = dspy.Predict(Judge)\n",
    "        self.improve_solution = dspy.Predict(Improve)\n",
    "\n",
    "    def forward(self, prompt):\n",
    "        # Analysis\n",
    "        problem_reflection = self.problem_reflector(prompt=prompt)\n",
    "        print(problem_reflection.rationale)\n",
    "\n",
    "\n",
    "        # Solution\n",
    "        solution = self.solution_generator(analysis=problem_reflection.rationale)\n",
    "        prev = solution.code_solution\n",
    "\n",
    "        # Candidate Solutions\n",
    "        solutions = \"\"\n",
    "        for i in range(3):\n",
    "            solution = self.solution_improver(analysis=problem_reflection.rationale, previous_attempt=prev)\n",
    "            print(solution)\n",
    "            solutions += solution.code_solution + \"\\n\"\n",
    "            prev = solution.code_solution\n",
    "\n",
    "        # Rank Candidates\n",
    "        top_ranked = self.rank_solutions(code_solutions=solutions)\n",
    "        print(top_ranked)\n",
    "\n",
    "        # Test Generation\n",
    "        test = self.generate_test(top_solution=top_ranked.top_ranked_code_solution)\n",
    "        print(test)\n",
    "\n",
    "        # Evaluate\n",
    "        valid = self.evaluate_solution(code=top_ranked.top_ranked_code_solution, test=test.test)\n",
    "        finished = \"true\" in str(valid.answer).lower()\n",
    "        attempts = 0\n",
    "\n",
    "        # Improve + evaluate loop\n",
    "        while not finished and attempts < 5:\n",
    "            top_ranked = self.improve_solution(code=top_ranked.top_ranked_code_solution, test=test.test)\n",
    "            valid = self.evaluate_solution(code=top_ranked.top_ranked_code_solution, test=test.test)\n",
    "            finished = \"true\" in str(valid.answer).lower()\n",
    "\n",
    "            attempts += 1\n",
    "\n",
    "        print(top_ranked.top_ranked_code_solution)\n",
    "\n",
    "        return top_ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: VALIDATION METRIC\n",
    "- As mentioned above, this validation metric is simplistic and doesn't fully capture 'success' here\n",
    "- The steps required to full run and evaluate code from every language is more than this module requires\n",
    "- So, we calculate our 'metric' as `cosine similarity` between strings\n",
    "- If the similarity is above 0.5, we can say that these two strings are close to similar\n",
    "- This gives us a very ROUGH idea of accuracy over our dataset (again, this is proof-of-concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(text):\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)\n",
    "\n",
    "def validate_code(example, pred, trace=None):\n",
    "    vec1 = text_to_vector(pred.top_ranked_code_solution)\n",
    "    vec2 = text_to_vector(example.completion)\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
    "    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    sim = 0.0\n",
    "\n",
    "    if denominator:\n",
    "        sim = float(numerator) / denominator\n",
    "\n",
    "    print(sim)\n",
    "\n",
    "    return sim > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7: TEST THE MODEL\n",
    "- Now that our model is built, we can test it before compiling with DSPy to see the raw output\n",
    "- We pass in a sample prompt for a Python Fibonacci program\n",
    "- We then print the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncompiled = NightwingCoder()\n",
    "\n",
    "output = uncompiled(prompt=\"Create an Python to calculate the Fibonacci sequence\")\n",
    "\n",
    "print(output.top_ranked_code_solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8: COMPILE\n",
    "- Now that everything is working, we can compile with our [Optimizer](https://dspy-docs.vercel.app/docs/building-blocks/optimizers)\n",
    "- [BootstrapFewShotWithRandomSearch](https://dspy-docs.vercel.app/docs/building-blocks/optimizers#how-do-i-use-an-optimizer) is the best optimizer for this use case\n",
    "- We pass in our `validate_code` metric method from above to return a boolean value validating each prediction against the data's ground truth\n",
    "- We set `max_bootstrapped_demos` to 4 to pull up to 4 examples back in to our prompt when optimizing\n",
    "- We then prompt our model again to see the difference in accuracy after compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(max_bootstrapped_demos=4, max_labeled_demos=4)\n",
    "\n",
    "teleprompter = BootstrapFewShotWithRandomSearch(metric=validate_code, **config)\n",
    "optimized_cot = teleprompter.compile(NightwingCoder(), trainset=train_dataset)\n",
    "\n",
    "output = optimized_cot(prompt=\"Create an Python to calculate the Fibonacci sequence\")\n",
    "\n",
    "print(output.top_ranked_code_solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 9: EVALUATE\n",
    "- Finally, we want to [evaluate](https://dspy-docs.vercel.app/docs/cheatsheet#dspy-evaluation) our model\n",
    "- We denote the `devset` and inputs, our validation metric from above, and set `num_threads` to 1\n",
    "- We then run the evaluation loop and analyze the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = Evaluate(devset=[x.with_inputs('prompt') for x in test_dataset], metric=validate_code, num_threads=1, display_progress=True, display_table=0)\n",
    "\n",
    "evaluate(optimized_cot)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
