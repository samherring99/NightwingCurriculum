{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 Project 3: ReAct\n",
    "Implement a basic RAG + ReAct pipeline to combine everything we've covered in this module so far\n",
    "We will use [llama-index](https://www.llamaindex.ai/) as a wrapper for our ReAct prompting to make life simpler for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install llama-index\n",
    "pip install llama_index.llms.llama_cpp\n",
    "pip install llama_index.embeddings.huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: IMPORTS\n",
    "- Import `VectorStoreIndex` and the directory reader from `llama-index`\n",
    "- We will be using LlamaCPP (this wraps `llama-cpp-python`)\n",
    "- We also need HuggingFaceEmbedding from `llama_index.embeddings` to build our embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.llms.llama_cpp.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: LOAD MODEL\n",
    "- We will be using a GGUF form of Llama-2-13B for this project at 4bit quantization (link below)\n",
    "- We set our context window to 3900 to allow some room for token generation\n",
    "- `n_gpu_layers` set to 1 is fine for this use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf\"\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    model_url=model_url,\n",
    "    model_path=None,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=1024,\n",
    "    context_window=3900,\n",
    "    generate_kwargs={},\n",
    "    model_kwargs={\"n_gpu_layers\": 1},\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: BUILD VECTOR STORE\n",
    "- First, we create our embedding model. We choose [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) as a standard embedding model on HF\n",
    "- We then load our data file (dante.txt) with the directory reader and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "data = SimpleDirectoryReader(input_dir=\"/documents/react\").load_data() # Place the text file(s) in the directory listed here\n",
    "index = VectorStoreIndex.from_documents(data, embed_model=embed_model)\n",
    "\n",
    "chat_engine = index.as_chat_engine(chat_mode=\"react\", llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: GENERATION\n",
    "- Now we can ask our model a question relating to the accompanying document\n",
    "- We can see based on it's response whether it is hallucinating the answer or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\n",
    "    \"Use the tool to answer what Dante's layers of hell are in concise descriptions?\"\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
