{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 Project 2: Word2Vec\n",
    "\n",
    "Implement [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) and play around with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: IMPORT THE NECESSARY LIBRARIES\n",
    "- We need a lot of stuff from `torch` for this project\n",
    "- Also the usual `numpy` imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.data import to_map_style_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import WikiText2, WikiText103\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: CONSTANTS AND HELPER METHODS\n",
    "- Here we set our constants for the CBOW and SkipGram models\n",
    "- We set the minimum word frequency for our vocab and maximum sequence length for collation (combination)\n",
    "- Collation methods below just prepare the text data in the appropriate format depending on our model choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CBOW_N_WORDS = 4\n",
    "SKIPGRAM_N_WORDS = 4\n",
    "\n",
    "MIN_WORD_FREQUENCY = 50\n",
    "MAX_SEQUENCE_LENGTH = 256\n",
    "\n",
    "EMBED_DIMENSION = 300\n",
    "EMBED_MAX_NORM = 1\n",
    "\n",
    "def collate_cbow(batch, text_pipeline):\n",
    "    batch_input, batch_output = [], []\n",
    "    for text in batch:\n",
    "        text_tokens_ids = text_pipeline(text)\n",
    "\n",
    "        if len(text_tokens_ids) < CBOW_N_WORDS * 2 + 1:\n",
    "            continue\n",
    "\n",
    "        if MAX_SEQUENCE_LENGTH:\n",
    "            text_tokens_ids = text_tokens_ids[:MAX_SEQUENCE_LENGTH]\n",
    "\n",
    "        for idx in range(len(text_tokens_ids) - CBOW_N_WORDS * 2):\n",
    "            token_id_sequence = text_tokens_ids[idx : (idx + CBOW_N_WORDS * 2 + 1)]\n",
    "            output = token_id_sequence.pop(CBOW_N_WORDS)\n",
    "            input_ = token_id_sequence\n",
    "            batch_input.append(input_)\n",
    "            batch_output.append(output)\n",
    "\n",
    "    batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
    "    batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
    "    return batch_input, batch_output\n",
    "\n",
    "def collate_skipgram(batch, text_pipeline):\n",
    "    batch_input, batch_output = [], []\n",
    "    for text in batch:\n",
    "        text_tokens_ids = text_pipeline(text)\n",
    "\n",
    "        if len(text_tokens_ids) < SKIPGRAM_N_WORDS * 2 + 1:\n",
    "            continue\n",
    "\n",
    "        if MAX_SEQUENCE_LENGTH:\n",
    "            text_tokens_ids = text_tokens_ids[:MAX_SEQUENCE_LENGTH]\n",
    "\n",
    "        for idx in range(len(text_tokens_ids) - SKIPGRAM_N_WORDS * 2):\n",
    "            token_id_sequence = text_tokens_ids[idx : (idx + SKIPGRAM_N_WORDS * 2 + 1)]\n",
    "            input_ = token_id_sequence.pop(SKIPGRAM_N_WORDS)\n",
    "            outputs = token_id_sequence\n",
    "\n",
    "            for output in outputs:\n",
    "                batch_input.append(input_)\n",
    "                batch_output.append(output)\n",
    "\n",
    "    batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
    "    batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
    "    return batch_input, batch_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: BUILD OUR DATASET AND DATALOADER\n",
    "- Using the provided `data.txt` file ([Norm MacDonald Wikipedia entry snippets](https://en.wikipedia.org/wiki/Norm_Macdonald)), we can create a Dataset class to iterate over the text in the file\n",
    "- Choosing our model type as CBOW by default\n",
    "- Using torch's basic English tokenizer to make life easy\n",
    "- We build our vocab using our tokenizer and our dataset\n",
    "- We will then build a DataLoader with a batch size of 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        self.source = open('./data.txt', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "    def __getitem__(self, idx) -> torch.Tensor:\n",
    "        source_sample = self.source[idx]\n",
    "        return source_sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "\n",
    "model_name = \"cbow\"\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\", language=\"en\")\n",
    "\n",
    "dataset = NormDataset()\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    map(tokenizer, dataset),\n",
    "    specials=[\"<unk>\"],\n",
    "    min_freq=MIN_WORD_FREQUENCY,\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "\n",
    "if model_name == \"cbow\":\n",
    "    collate_fn = collate_cbow\n",
    "elif model_name == \"skipgram\":\n",
    "    collate_fn = collate_skipgram\n",
    "else:\n",
    "    raise ValueError(\"Choose model from: cbow, skipgram\")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=96,\n",
    "    shuffle=True,\n",
    "    collate_fn=partial(collate_fn, text_pipeline=text_pipeline),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: BUILD OUR MODELS\n",
    "- Here we define our two models, CBOW and SkipGram\n",
    "- CBOW uses all context words arround a 'middle term' to predict the term in context\n",
    "- SkipGram is only given one word as context, and asked to predict the next word in the sentence\n",
    "- Therefore, we rewuire different models because the inputs are shaped differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW_Model(nn.Module):\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super(CBOW_Model, self).__init__()\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=EMBED_DIMENSION,\n",
    "            max_norm=EMBED_MAX_NORM,\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=EMBED_DIMENSION,\n",
    "            out_features=vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.embeddings(inputs)\n",
    "        x = x.mean(axis=1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class SkipGram_Model(nn.Module):\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super(SkipGram_Model, self).__init__()\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=EMBED_DIMENSION,\n",
    "            max_norm=EMBED_MAX_NORM,\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=EMBED_DIMENSION,\n",
    "            out_features=vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs_):\n",
    "        x = self.embeddings(inputs_)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: BUILD OUR TRAINER CLASS\n",
    "- We don't expressly need this, but it makes life easy in the final step to train this model\n",
    "- The below code contains the classic training code for torch models, nothing really fancy going on here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        epochs,\n",
    "        train_dataloader,\n",
    "        train_steps,\n",
    "        val_dataloader,\n",
    "        val_steps,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        lr_scheduler,\n",
    "        device,\n",
    "    ):  \n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.train_steps = train_steps\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.val_steps = val_steps\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.device = device\n",
    "\n",
    "        self.loss = {\"train\": [], \"val\": []}\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            self._train_epoch()\n",
    "            self._validate_epoch()\n",
    "            print(\n",
    "                \"Epoch: {}/{}, Train Loss={:.5f}, Val Loss={:.5f}\".format(\n",
    "                    epoch + 1,\n",
    "                    self.epochs,\n",
    "                    self.loss[\"train\"][-1],\n",
    "                    self.loss[\"val\"][-1],\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.lr_scheduler.step()\n",
    "\n",
    "    def _train_epoch(self):\n",
    "        self.model.train()\n",
    "        running_loss = []\n",
    "\n",
    "        for i, batch_data in enumerate(self.train_dataloader, 1):\n",
    "            inputs = batch_data[0].to(self.device)\n",
    "            labels = batch_data[1].to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss.append(loss.item())\n",
    "\n",
    "            if i == self.train_steps:\n",
    "                break\n",
    "\n",
    "        epoch_loss = np.mean(running_loss)\n",
    "        self.loss[\"train\"].append(epoch_loss)\n",
    "\n",
    "    def _validate_epoch(self):\n",
    "        self.model.eval()\n",
    "        running_loss = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, batch_data in enumerate(self.val_dataloader, 1):\n",
    "                inputs = batch_data[0].to(self.device)\n",
    "                labels = batch_data[1].to(self.device)\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "\n",
    "                running_loss.append(loss.item())\n",
    "\n",
    "                if i == self.val_steps:\n",
    "                    break\n",
    "\n",
    "        epoch_loss = np.mean(running_loss)\n",
    "        self.loss[\"val\"].append(epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: TRAIN THE MODEL\n",
    "- Now we can finally train Word2Vec from scratch\n",
    "- We set the model type and initialize, as well as initializing the optimizer and the learning rate scheduler\n",
    "- We chose 100 epochs and a LR of 0.025 arbitrarily for this example, as well as the train and validation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab.get_stoi())\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "model_class = CBOW_Model\n",
    "model = model_class(vocab_size=vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_class = optim.Adam\n",
    "optimizer = optimizer_class(model.parameters(), lr=0.025)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "lr_lambda = lambda epoch: (epochs - epoch) / epochs\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda, verbose=True)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    epochs=100,\n",
    "    train_dataloader=dataloader,\n",
    "    train_steps=10,\n",
    "    val_dataloader=dataloader,\n",
    "    val_steps=10,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Training finished.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
