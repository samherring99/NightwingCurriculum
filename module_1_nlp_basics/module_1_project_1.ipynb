{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 Project 1: Tokenization\n",
    "\n",
    "Recreate the GPT tokenizer following minBPE exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: UTF-8 ENCODING \n",
    "- Intro to UTF-8 encoding for text and mapping byte encodings in a list to characters\n",
    "- Display the original text and the 'tokenized' form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The Tokenizer is a necessary and pervasive component of Large Language Models (LLMs), where it translates between strings and tokens (text chunks). Tokenizers are a completely separate stage of the LLM pipeline: they have their own training sets, training algorithms (Byte Pair Encoding), and after training implement two fundamental functions: encode() from strings to tokens, and decode() back from tokens to strings. In this lecture we build from scratch the Tokenizer used in the GPT series from OpenAI. In the process, we will see that a lot of weird behaviors and problems of LLMs actually trace back to tokenization. We'll go through a number of these issues, discuss why tokenization is at fault, and why someone out there ideally finds a way to delete this stage entirely.\"\"\"\n",
    "\n",
    "tokens = text.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens))\n",
    "print(text)\n",
    "print(\"Text: \" + str(len(text)) + \" characters\")\n",
    "print(tokens)\n",
    "print(\"Tokens: \" + str(len(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: GET PAIR COUNTS\n",
    "- Iterate over the pairs of byte encodings to determine which pair happens the most frequently in the given text\n",
    "- Output the pairs as keys with their count as the value in a dicitonary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_counts(token_ids):\n",
    "    counts = {}\n",
    "    for pair in zip(token_ids, token_ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "pairs = get_pair_counts(tokens)\n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 'max' to get the most commonly occurring pair\n",
    "top_pair = max(pairs, key=pairs.get)\n",
    "print(top_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: ADD NEW TOKEN AS MERGE\n",
    "- We have 256 indices of pairs from UTF-8 encoding.\n",
    "- If we want to merge common pairs (liek the one found above) we need to add new indices to the list\n",
    "- The below method will take a list of token ids, a pair, and a new id, and will merge all occurrences of the pair into the new index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method merges a pair into a list of token IDs and assigns it the given index\n",
    "def new_token(token_ids, pair, index):\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(token_ids):\n",
    "        if i < len(token_ids) - 1 and token_ids[i] == pair[0] and token_ids[i+1] == pair[1]:\n",
    "            new_ids.append(index)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(token_ids[i])\n",
    "            i += 1\n",
    "    return new_ids\n",
    "\n",
    "print(new_token(tokens, (32, 116), 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: PERFORM MERGE AND BUILD VOCAB\n",
    "- In order to go 'backwards' from tokens to text, we need to build a mapping of byte pairs to characters\n",
    "- We do this by building a 'vocab' to use as a mapping reference\n",
    "- The below code will build this vocab after performing the merge for a given # of steps (adding new tokens X amount of times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 276 # We want to perform 20 merges since our original index count is 256, chosen arbitrarily\n",
    "num_merges = vocab_size - 256\n",
    "token_ids = list(tokens)\n",
    "\n",
    "# Loop to perform the merge. Steps are: get pair counts, find the max occurrence, merge it into a new ID, repeat N times\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    stats = get_pair_counts(token_ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    index = 256 + i\n",
    "    print(f\"Merging {pair} into new token {index}\")\n",
    "    token_ids = new_token(token_ids, pair, index)\n",
    "    merges[pair] = index\n",
    "\n",
    "# Compression ratio compared to original token length\n",
    "print(f\"Compression ratio {len(tokens) / len(token_ids):.2f}X\")\n",
    "\n",
    "# Building our vocab for decoding\n",
    "vocab = {index: bytes([index]) for index in range(256)}\n",
    "for (p0, p1), index in merges.items():\n",
    "    vocab[index] = vocab[p0] + vocab[p1]\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: ENCODING AND DECODING\n",
    "- Decoding step is relatively straightforward, just concatenate bytes into a string, and decode from UTF-8\n",
    "- Be sure to use `errors='replace'` with the call to 'decode' to ensure non-UTF-8 characters get handled appropriately\n",
    "- The use of our previously built 'vocab' here helps us go from merged token pairs to their character representations.\n",
    "- Encoding process uses our 'get_pair_counts' and 'new_token' methods to get the 'next' index that was merged into our encoding map, the same order that we used to build our vocab initially.\n",
    "- Doing this allows us to go from individual token representations to merged pairs as tokens that are consistent with the way we built the initial vocab and merges dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of token IDs, return the text representation\n",
    "def decode(token_ids):\n",
    "    tokens = b\"\".join(vocab[index] for index in token_ids)\n",
    "    text = tokens.decode(\"utf-8\", errors='replace')\n",
    "    return text\n",
    "\n",
    "# Given a string of text, encode into tokens using our BPE algorithm\n",
    "def encode(text):\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    while len(tokens) >= 2:\n",
    "        stats = get_pair_counts(tokens)\n",
    "        pair = min(stats, key= lambda x: merges.get(x, float(\"inf\")))\n",
    "        if pair not in merges:\n",
    "            break\n",
    "        index = merges[pair]\n",
    "        tokens = new_token(tokens, pair, index)\n",
    "    return tokens\n",
    "\n",
    "print(decode(encode(\"Hello! this is a test string!\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: PUT IT ALL TOGETHER\n",
    "- Put everything above together in a single class, and test out model training and encoding/decoding\n",
    "- At this point we have re-created a simple tokenizer following minBPE and understand the fundamentals around tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"Base class for Tokenizer\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.merges = {} # (int, int) -> int\n",
    "        self.vocab = {}\n",
    "\n",
    "    # Get pair counts\n",
    "    def get_pair_counts(self, token_ids):\n",
    "        counts = {}\n",
    "        for pair in zip(token_ids, token_ids[1:]):\n",
    "            counts[pair] = counts.get(pair, 0) + 1\n",
    "        return counts\n",
    "\n",
    "    # Merge pair into new token with index\n",
    "    def new_token(self, token_ids, pair, index):\n",
    "        new_ids = []\n",
    "        i = 0\n",
    "        while i < len(token_ids):\n",
    "            if i < len(token_ids) - 1 and token_ids[i] == pair[0] and token_ids[i+1] == pair[1]:\n",
    "                new_ids.append(index)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_ids.append(token_ids[i])\n",
    "                i += 1\n",
    "        return new_ids\n",
    "\n",
    "    # Train a vocab of a given size form the given text\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        assert vocab_size >= 256\n",
    "        num_merges = vocab_size - 256\n",
    "\n",
    "        text_bytes = text.encode(\"utf-8\")\n",
    "        token_ids = list(text_bytes)\n",
    "\n",
    "        merges = {}\n",
    "        vocab = {index: bytes([index]) for index in range(256)}\n",
    "\n",
    "        for i in range(num_merges):\n",
    "            pair_counts = self.get_pair_counts(token_ids)\n",
    "            pair = max(pair_counts, key=pair_counts.get)\n",
    "            index = 256 + i\n",
    "            token_ids = self.new_token(token_ids, pair, index)\n",
    "            merges[pair] = index\n",
    "            vocab[index] = vocab[pair[0]] + vocab[pair[1]]\n",
    "        \n",
    "        self.merges = merges\n",
    "        self.vocab = vocab\n",
    "\n",
    "    # Given a string of text, encode into BPE representation (list of integers)\n",
    "    def encode(self, text):\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        while len(tokens) >= 2:\n",
    "            stats = self.get_pair_counts(tokens)\n",
    "            pair = min(stats, key= lambda x: self.merges.get(x, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            index = self.merges[pair]\n",
    "            tokens = self.new_token(tokens, pair, index)\n",
    "        return tokens\n",
    "\n",
    "    # Given BPE representation (list of integers), return the string representation\n",
    "    def decode(self, token_ids):\n",
    "        tokens = b\"\".join(self.vocab[index] for index in token_ids)\n",
    "        text = tokens.decode(\"utf-8\", errors='replace')\n",
    "        return text\n",
    "\n",
    "token = Tokenizer()\n",
    "text = \"Norman Gene Macdonald[i] (October 17, 1959[ii] September 14, 2021) was a Canadian stand-up comedian, actor, and writer whose style was characterized by deadpan delivery and the use of folksy, old-fashioned turns of phrase.[1][2][3] He appeared in many films and was a regular guest on late-night talk shows, where he became known for his chaotic, yet understated style of comedy.[4] Many critics and fellow comedians considered him to be the ultimate talk show guest, while prominent late-night figure David Letterman regarded him as 'the best' of stand-up comedians.[5][6] Earlier in his career, Macdonald's first work on television included writing for such comedies as Roseanne and The Dennis Miller Show. In 1993, Macdonald was hired as a writer and cast member on Saturday Night Live (SNL), spending a total of five seasons on the series, which included anchoring the show's Weekend Update segment for three and a half seasons.[7] He was removed as host of SNL's Weekend Update in 1998, allegedly for relentlessly mocking OJ Simpson during his murder trial, offending producer Don Ohlmeyer who was a close friend of Simpson.[8][9] After being fired from SNL, he wrote and starred in the 1998 film Dirty Work and headlined his own sitcom The Norm Show from 1999 to 2001. Macdonald was also a voice actor, and provided voice acting roles for Family Guy, The Fairly OddParents, Mike Tyson Mysteries, The Orville, and the Dr. Dolittle films.\"\n",
    "\n",
    "token.train(text, 286) # We want to do 30 merges, as an example\n",
    "print(token.merges)\n",
    "print(token.decode(token.encode(\"Hello! this is a text string!\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
