{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 Project 3: BERT\n",
    "\n",
    "Implement BERT and play around with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 0: IMPORT THE NECESSARY LIBRARIES\n",
    "- We need Torch, transformers, and numpy for this implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "import transformers\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import itertools\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: DATA LOADING AND PREPROCESSING\n",
    "- Here, the provided data is two text files, one with lines from a conversation in a movie, and one with each line containing the line numbers of a given conversation\n",
    "- We are splitting on the special delimiter \" +++$+++ \" as the examples in the data file are delimited with this\n",
    "- We grab the last piece of splitting on the above delimiter to get the text or line numbers\n",
    "- Lastly we build question/answer pairs using the line numbers from the conversation data file\n",
    "- This gives us a solid dataset of question and answer pairs to train our model on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max input tokens\n",
    "MAX_LEN = 64\n",
    "\n",
    "corpus_movie_conv = './movie_data/movie_conversations.txt'\n",
    "corpus_movie_lines = './movie_data/movie_lines.txt'\n",
    "with open(corpus_movie_conv, 'r', encoding='iso-8859-1') as c:\n",
    "    conv = c.readlines()\n",
    "with open(corpus_movie_lines, 'r', encoding='iso-8859-1') as l:\n",
    "    lines = l.readlines()\n",
    "\n",
    "lines_dic = {}\n",
    "for line in lines:\n",
    "    objects = line.split(\" +++$+++ \")\n",
    "    lines_dic[objects[0]] = objects[-1]\n",
    "\n",
    "pairs = []\n",
    "for con in conv:\n",
    "    ids = eval(con.split(\" +++$+++ \")[-1])\n",
    "    for i in range(len(ids)):\n",
    "        qa_pairs = []\n",
    "        \n",
    "        if i == len(ids) - 1:\n",
    "            break\n",
    "\n",
    "        first = lines_dic[ids[i]].strip()  \n",
    "        second = lines_dic[ids[i+1]].strip() \n",
    "\n",
    "        qa_pairs.append(' '.join(first.split()[:MAX_LEN]))\n",
    "        qa_pairs.append(' '.join(second.split()[:MAX_LEN]))\n",
    "        pairs.append(qa_pairs)\n",
    "\n",
    "print(pairs[12]) # Printing a random pair as an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: DATA PREPARATION\n",
    "- Once we have the question/answer pairs as displayed above, we can generate the data files to feed into our model\n",
    "- BertWordPieceTokenizer from `transformers` needs to process a list of file paths that represent our corpus\n",
    "- So we split our conversation question and answers into text files with ~10000 characters in each one\n",
    "- This should result in around ~21 text files in `./data` if using the provided datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('./data')\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in [x[0] for x in pairs]:\n",
    "    text_data.append(sample)\n",
    "\n",
    "    # Each text file should be ~10000 characters\n",
    "    if len(text_data) == 10000:\n",
    "        with open(f'./data/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "\n",
    "paths = [str(x) for x in Path('./data').glob('**/*.txt')]\n",
    "\n",
    "# Printing the first path as an example\n",
    "print(paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: TOKENIZER\n",
    "- We will be using BertWordPieceTokenizer from `transformers`\n",
    "- The use of a pre-built tokenizer here is so we don't have to re-implement the BERT tokenizer\n",
    "- Wordpieces prefix is an important concept here not covered in the tokenizer piece of this module\n",
    "- Special tokens are used to denote the start and end of sentences, as well as a `[MASK]` token for use in masking\n",
    "- We will train the tokenizer with a vocab size of 30000\n",
    "- We will save the vocab to `./bert-it-1/bert-it-vocab.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=False,\n",
    "    strip_accents=False,\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "tokenizer.train( \n",
    "    files=paths,\n",
    "    vocab_size=30_000, \n",
    "    min_frequency=5,\n",
    "    limit_alphabet=1000, \n",
    "    wordpieces_prefix='##',\n",
    "    special_tokens=['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']\n",
    "    )\n",
    "\n",
    "os.mkdir('./bert-it-1')\n",
    "tokenizer.save_model('./bert-it-1', 'bert-it')\n",
    "tokenizer = BertTokenizer.from_pretrained('./bert-it-1/bert-it-vocab.txt', local_files_only=True)\n",
    "\n",
    "# Assert we have our vocab file created correctly\n",
    "print(os.path.exists(\"./bert-it-1/bert-it-vocab.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: DATASET CLASS WITH TOKENIZATION\n",
    "- We use a wrapper class here to handle the dataset loading and tokenization pieces with one call\n",
    "- This class will be wrapped in a torch DataLoader during training and enumerates at top level with `__getitem__`\n",
    "- When a new item is produced, two sentences are chosen a 'first' and a 'next' with the goal of predicting the likelihood the 'first' sentence the 'next' one\n",
    "- There is a 50% chance the returned 'next' sentence will actually follow the given 'first' sentence\n",
    "- BERT has a 15% likelihood of masking a random word within each sentence, and an additional 10% chance of choosing a random replacement for that word, instead of the token '[MASK]'\n",
    "- This comes into play later with Masked Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, data_pair, tokenizer, seq_len=64):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.corpus_lines = len(data_pair)\n",
    "        self.lines = data_pair\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.corpus_lines\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        t1, t2, is_next_label = self.get_sent(item)\n",
    "\n",
    "        t1_random, t1_label = self.random_word(t1)\n",
    "        t2_random, t2_label = self.random_word(t2)\n",
    "\n",
    "        t1 = [self.tokenizer.vocab['[CLS]']] + t1_random + [self.tokenizer.vocab['[SEP]']]\n",
    "        t2 = t2_random + [self.tokenizer.vocab['[SEP]']]\n",
    "        t1_label = [self.tokenizer.vocab['[PAD]']] + t1_label + [self.tokenizer.vocab['[PAD]']]\n",
    "        t2_label = t2_label + [self.tokenizer.vocab['[PAD]']]\n",
    "\n",
    "        segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]\n",
    "        bert_input = (t1 + t2)[:self.seq_len]\n",
    "        bert_label = (t1_label + t2_label)[:self.seq_len]\n",
    "        padding = [self.tokenizer.vocab['[PAD]'] for _ in range(self.seq_len - len(bert_input))]\n",
    "        bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)\n",
    "\n",
    "        output = {\"bert_input\": bert_input,\n",
    "                    \"bert_label\": bert_label,\n",
    "                    \"segment_label\": segment_label,\n",
    "                    \"is_next\": is_next_label}\n",
    "\n",
    "        return {key: torch.tensor(value) for key, value in output.items()}\n",
    "\n",
    "    def random_word(self, sentence):\n",
    "        tokens = sentence.split()\n",
    "        output_label = []\n",
    "        output = []\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            prob = random.random()\n",
    "            token_id = self.tokenizer(token)['input_ids'][1:-1]\n",
    "\n",
    "            if prob < 0.15:\n",
    "                prob /= 0.15\n",
    "                if prob < 0.8:\n",
    "                    for i in range(len(token_id)):\n",
    "                        output.append(self.tokenizer.vocab['[MASK]'])\n",
    "                elif prob < 0.9:\n",
    "                    for i in range(len(token_id)):\n",
    "                        output.append(random.randrange(len(self.tokenizer.vocab)))\n",
    "                else:\n",
    "                    output.append(token_id)\n",
    "\n",
    "                output_label.append(token_id)\n",
    "            else:\n",
    "                output.append(token_id)\n",
    "                for i in range(len(token_id)):\n",
    "                    output_label.append(0)\n",
    "\n",
    "        output = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output]))\n",
    "        output_label = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output_label]))\n",
    "        assert len(output) == len(output_label)\n",
    "        return output, output_label\n",
    "\n",
    "    def get_sent(self, index):\n",
    "        t1, t2 = self.get_corpus_line(index)\n",
    "\n",
    "        if random.random() > 0.5:\n",
    "            return t1, t2, 1\n",
    "        else:\n",
    "            return t1, self.get_random_line(), 0\n",
    "\n",
    "    def get_corpus_line(self, item):\n",
    "        return self.lines[item][0], self.lines[item][1]\n",
    "\n",
    "    def get_random_line(self):\n",
    "        return self.lines[random.randrange(len(self.lines))][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: TEST THE DATASET GENERATION\n",
    "- We want to confirm that our Tokenizer works with our Dataset class.\n",
    "- So we build a `train_data` variable to store our data given the Q/A pairs we generated earlier.\n",
    "- We then display the 'next' data piece in the data loader (random) and we choose a random piece from our Dataset class to see it's value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = BERTDataset(\n",
    "   pairs, seq_len=MAX_LEN, tokenizer=tokenizer)\n",
    "train_loader = DataLoader(\n",
    "   train_data, batch_size=32, shuffle=True, pin_memory=True)\n",
    "sample_data = next(iter(train_loader))\n",
    "print(sample_data)\n",
    "print(train_data[random.randrange(len(train_data))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: POSITIONAL EMBEDDING\n",
    "- BERT works with 3 types of embedding: Token, Positional, and Segment embedding\n",
    "- Token embedding is a simple lookup of size `vocab_size` with each embedding having size `embed_size`\n",
    "- Positional embedding works differently, where `sin` and `cos` are used to capture the positional information in a vector with a given sequence length\n",
    "- Segment embedding is yet another simple lookup of whether or not a token belongs to sentence A or sentence B, with each embedding having size `embed_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, max_len=128):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, hidden_size).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        for pos in range(max_len):   \n",
    "            for i in range(0, hidden_size, 2):   \n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/hidden_size)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/hidden_size)))\n",
    "\n",
    "        self.pe = pe.unsqueeze(0)   \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7: BERT EMBEDDING\n",
    "- Combine the PositionalEmbedding class with the Token and Segment embeddings using `torch.nn.Embedding`\n",
    "- Add in a dropout later using a dropout of 0.1\n",
    "- When this BERTEmbedding class is utilized, it will combine the Token, Positional, and Segment embeddings and returns the result after dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, seq_len=64, dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.token = torch.nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.segment = torch.nn.Embedding(3, embed_size, padding_idx=0)\n",
    "        self.position = PositionalEmbedding(hidden_size=embed_size, max_len=seq_len)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "       \n",
    "    def forward(self, sequence, segment_label):\n",
    "        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7: MUTLI HEADED ATTENTION\n",
    "- Multi Headed Attention uses a scaled dot product to perform the 'attention' step multiple times in parallel\n",
    "- The 'attention' step here leverages our positional/token/segment encoding to calculate 'soft' weights for each word in a given sentence\n",
    "- This loosely corresponds to the relationship between a given word (our query), the possible important words in context that are related to this word (our key) and the position of these possible matches (our value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, heads, hidden_size, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        \n",
    "        assert hidden_size % heads == 0\n",
    "        self.d_k = hidden_size // heads\n",
    "        self.heads = heads\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.query = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.key = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.value = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.output_linear = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)        \n",
    "        value = self.value(value)   \n",
    "\n",
    "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
    "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        \n",
    "        scores = torch.matmul(query, key.permute(0, 1, 3, 2)) / math.sqrt(query.size(-1))\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)    \n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)           \n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        context = torch.matmul(weights, value)\n",
    "        context = context.permute(0, 2, 1, 3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
    "\n",
    "        return self.output_linear(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8: FEED FORWARD LAYER\n",
    "- We need a generalized Feed Forward layer in our model to manage the weights generated from the attention layer created above\n",
    "- This layer is two fully connected layers, with input size `hidden_size`\n",
    "- It also uses a dropout layer (of 0.1) and GELU activation after the first fully connected layer, and then returns the ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, middle_dim=2048, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(hidden_size, middle_dim)\n",
    "        self.fc2 = torch.nn.Linear(middle_dim, hidden_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.activation = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.fc1(x))\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8: ENCODER LAYER\n",
    "- We use both the FeedForward and MultiHeadedAttention classes created above to build our Encoder layer\n",
    "- This serves as the main 'building block' for BERT architecture, there are 12 of them in this example\n",
    "- The encoder layer gives a predefined `hidden_size` (the size of the attention and FF layer inputs/outputs) and a set number of attention heads (to be run in parallel)\n",
    "- Dropout is applied after the attention later, and then again after the feed forward layer\n",
    "- The embeddings of each input sentence are normalized and fed into the feed forward layer before the forward pass\n",
    "- The output is the normalized with the attention results again before returning output from the Encoder layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        hidden_size=768,\n",
    "        heads=12, \n",
    "        feed_forward_hidden=768 * 4, \n",
    "        dropout=0.1\n",
    "        ):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.layernorm = torch.nn.LayerNorm(hidden_size)\n",
    "        self.self_multihead = MultiHeadedAttention(heads, hidden_size)\n",
    "        self.feed_forward = FeedForward(hidden_size, middle_dim=feed_forward_hidden)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, embeddings, mask):\n",
    "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
    "        interacted = self.layernorm(interacted + embeddings)\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        encoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 9: BERT BASE MODULE\n",
    "- We use the BERTEmbedding and EncoderLayer classes created above to build our BERT base\n",
    "- Given that we have set 12 layers below, 12 Encoder layers are created after our embedding is complete\n",
    "- This is not good enough for a standalone language model just yet\n",
    "- We need to incorporate our masking and a prediction if a given sentence A is followed by a given sentence B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=768, n_layers=12, heads=12, dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.heads = heads\n",
    "\n",
    "        self.feed_forward_hidden = hidden_size * 4\n",
    "\n",
    "        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=hidden_size)\n",
    "\n",
    "        self.encoder_blocks = torch.nn.ModuleList(\n",
    "            [EncoderLayer(hidden_size, heads, hidden_size * 4, dropout) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, segment_info):\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "\n",
    "        x = self.embedding(x, segment_info)\n",
    "\n",
    "        for encoder in self.encoder_blocks:\n",
    "            x = encoder.forward(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 9: NEXT SENTENCE PREDICTION\n",
    "- The first piece we need to complete our LM is Next Sentence Prediction\n",
    "- This simply consists of a binary classifier and softmax output for (\"YES\" or \"NO) - which denotes whether sentence B follows sentence A when provided two sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextSentencePrediction(torch.nn.Module):\n",
    "    def __init__(self, hidden):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(hidden, 2)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 10: MASKED LANGUAGE MODEL\n",
    "- Masked Language Modeling (MLM) is used with the ouputs of our BERT module to determine the probabilites of a masked token being it's predicted value\n",
    "- This is another simple linear layer and a softmax on the output, with the size being `vocab_size`\n",
    "- This outputs the probabilities of all tokens in our vocab being the masked token in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLanguageModel(torch.nn.Module):\n",
    "    def __init__(self, hidden, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(hidden, vocab_size)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 11: BUILD OUR LANGUAGE MODEL\n",
    "- Now we can finally build our LM\n",
    "- We combine the BERT module with Next Sentence Prediction and Masked Language Modeling to output the probablities of the masked token and the 'net' sentence being the following sentence in the corpus\n",
    "- We return these probabilities as outputs to calculate our loss and help train our BERTLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTLM(torch.nn.Module):\n",
    "    def __init__(self, bert: BERT, vocab_size):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.next_sentence = NextSentencePrediction(self.bert.hidden_size)\n",
    "        self.mask_lm = MaskedLanguageModel(self.bert.hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, segment_label):\n",
    "        x = self.bert(x, segment_label)\n",
    "        return self.next_sentence(x), self.mask_lm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 12: OPTIMIZATION\n",
    "- We will use a learning rate scheduler and optimizer to train our model\n",
    "- We will use Adam for our optimizer with a learning rate of 1e-4 to start\n",
    "- We will also use a weight decay of 0.01\n",
    "- Our optimizer class has a top level method `step_and_update_lr` that is used to step through our training sequence with the appropriate learning rate reductions and optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledOptim():\n",
    "    def __init__(self, optimizer, hidden_size, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(hidden_size, -0.5)\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        self.n_current_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 12: MODEL TRAINER\n",
    "- Now we can combine everything into a final class, our model trainer\n",
    "- This class isnt explicitly necessary, but for re-usability we can utilize this class to abstract away a lot of the common code used in training models\n",
    "- This class contains our BERT model, our DataLoader, our optimizer, and an iteration method to step through a batch in our training sequence\n",
    "- We also use a loss function called negative log likelihood loss to calculate the error in the outputs from the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTTrainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model, \n",
    "        train_dataloader, \n",
    "        test_dataloader=None, \n",
    "        lr= 1e-4,\n",
    "        weight_decay=0.01,\n",
    "        betas=(0.9, 0.999),\n",
    "        warmup_steps=10000,\n",
    "        log_freq=10,\n",
    "        device='cpu'\n",
    "        ):\n",
    "\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        self.train_data = train_dataloader\n",
    "        self.test_data = test_dataloader\n",
    "\n",
    "        self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        self.optim_schedule = ScheduledOptim(\n",
    "            self.optim, self.model.bert.hidden_size, n_warmup_steps=warmup_steps\n",
    "            )\n",
    "\n",
    "        self.criterion = torch.nn.NLLLoss(ignore_index=0)\n",
    "        self.log_freq = log_freq\n",
    "        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
    "    \n",
    "    def train(self, epoch):\n",
    "        self.iteration(epoch, self.train_data)\n",
    "\n",
    "    def test(self, epoch):\n",
    "        self.iteration(epoch, self.test_data, train=False)\n",
    "\n",
    "    def iteration(self, epoch, data_loader, train=True):\n",
    "        avg_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_element = 0\n",
    "        \n",
    "        mode = \"train\" if train else \"test\"\n",
    "\n",
    "        for i, data in enumerate(data_loader):\n",
    "            data = {key: value.to(self.device) for key, value in data.items()}\n",
    "            next_sent_output, mask_lm_output = self.model.forward(data[\"bert_input\"], data[\"segment_label\"])\n",
    "            next_loss = self.criterion(next_sent_output, data[\"is_next\"])\n",
    "            mask_loss = self.criterion(mask_lm_output.transpose(1, 2), data[\"bert_label\"])\n",
    "            loss = next_loss + mask_loss\n",
    "\n",
    "            if train:\n",
    "                self.optim_schedule.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim_schedule.step_and_update_lr()\n",
    "\n",
    "            correct = next_sent_output.argmax(dim=-1).eq(data[\"is_next\"]).sum().item()\n",
    "            avg_loss += loss.item()\n",
    "            total_correct += correct\n",
    "            total_element += data[\"is_next\"].nelement()\n",
    "\n",
    "            post_fix = {\n",
    "                \"epoch\": epoch,\n",
    "                \"iter\": i,\n",
    "                \"avg_loss\": avg_loss / (i + 1),\n",
    "                \"avg_acc\": total_correct / total_element * 100,\n",
    "                \"loss\": loss.item()\n",
    "            }\n",
    "\n",
    "            if i % self.log_freq == 0:\n",
    "                print(str(post_fix))\n",
    "        print(\n",
    "            f\"EP{epoch}, {mode}: \\\n",
    "            avg_loss={avg_loss / len(enumerate(data_loader))}, \\\n",
    "            total_acc={total_correct * 100.0 / total_element}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 12: TRAIN THE MODEL\n",
    "- In the final step here, we train our model on our provided data\n",
    "- After setting up the BERTDataset and data loader (again), we can create our model base\n",
    "- Given this model base, we can build our BERTLM language model and trainer class\n",
    "- Defining 20 epochs, we iterate through them and iterate through our batches of training data in each pass\n",
    "- Now we will have implemented and trained BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = BERTDataset(\n",
    "   pairs, seq_len=MAX_LEN, tokenizer=tokenizer)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "   train_data, batch_size=32, shuffle=True, pin_memory=True)\n",
    "\n",
    "bert_model = BERT(\n",
    "  vocab_size=len(tokenizer.vocab),\n",
    "  hidden_size=768,\n",
    "  n_layers=2,\n",
    "  heads=12,\n",
    "  dropout=0.1\n",
    ")\n",
    "\n",
    "bert_lm = BERTLM(bert_model, len(tokenizer.vocab))\n",
    "bert_trainer = BERTTrainer(bert_lm, train_loader, device='cpu')\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  bert_trainer.train(epoch)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
