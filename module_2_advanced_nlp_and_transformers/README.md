# Module 2: Advanced NLP and Transformers

## Introduction

[TODO] Introduction on what we aim to cover here and why

## Projects

### Project 1 - Generative Pretrained Transformers

[TODO] Project 1 description brief

https://github.com/karpathy/ng-video-lecture - reference etc

#### Goals: 

- Understand self-attention mechanisms
- Study the Transformer architecture

#### Readings:
- ðŸ“– [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- ðŸ“– [Transformers Introduction](https://www.turing.com/kb/brief-introduction-to-transformers-and-their-power)
- ðŸ“– [Attention Mechanism](https://machinelearningmastery.com/the-transformer-attention-mechanism/)

#### Videos:
- ðŸ“º [Karpathy GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY)

### Project 2 - Text-to-text Transfer Transformers

[TODO] Project 2 description brief

#### Goals: 

- Explore XLNet, T5, and other variants
- Understand their arcitectures and differences from classic GPT

#### Readings:
- ðŸ“– [XLNet](https://towardsdatascience.com/what-is-xlnet-and-why-it-outperforms-bert-8d8fce710335)
- ðŸ“– [T5 Introduction](https://blog.research.google/2020/02/exploring-transfer-learning-with-t5.html)
- ðŸ“– [T5 Deep Dive](https://cameronrwolfe.substack.com/p/t5-text-to-text-transformers-part)

#### Videos:
- ðŸ“º [T5 Continued](https://www.youtube.com/watch?v=91iLu6OOrwk)

### Project 3 - Mixture of Experts

[TODO] Project 3 description brief

#### Goals: 

- Understand the MoE architecture and its applications.
- Study recent advancements and variants.

#### Readings:
- ðŸ“– [Mixture of Experts Introduction](https://machinelearningmastery.com/mixture-of-experts/)
- ðŸ“– [Mixture of Experts Explained](https://www.tensorops.ai/post/what-is-mixture-of-experts-llm)
- ðŸ“– [Mixture of Experts Paper](https://arxiv.org/abs/1312.4314)

#### Videos:
- ðŸ“º [Mixture of Experts Explained](https://www.youtube.com/watch?v=mwO6v4BlgZQ)

### Project 4 - Finetuning

[TODO] Project 4 description brief

#### Goals: 

- Learn techniques for fine-tuning pre-trained models for specific tasks
- Understand model distillation: compressing large models into smaller ones
- Parameter Efficient Finetuning - PeFT
- Learn how LoRA works abnd how to implement it from scratch


#### Readings:
- ðŸ“– [Finetuning Introduction](https://www.turing.com/resources/finetuning-large-language-models)
- ðŸ“– [LoRA](https://towardsdatascience.com/understanding-lora-low-rank-adaptation-for-finetuning-large-models-936bce1a07c6)
- ðŸ“– [LLM Distillation](https://snorkel.ai/llm-distillation-techniques-to-explode-in-importance-in-2024/)

#### Videos:
- ðŸ“º [Finetuning with examples](https://www.youtube.com/watch?v=eC6Hd1hFvos)

### Conclusion:

[TODO] Cover everything we should have learned in these projects
