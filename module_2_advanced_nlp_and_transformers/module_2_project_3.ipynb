{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 Project 3: Mixture of Experts\n",
    "- Implement a [MoE model](https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch) using everything that has been covered in previous modules so far\n",
    "- This is pretty much a standard transformer model but with the added benefit of having multiple, smaller feed forward networks instead of the traditional FF layer, and a Router / Gating network to determine which experts to send each token to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: IMPORTS\n",
    "- We need the usual `torch` imports, as well as `os` and `re`\n",
    "- We use `requests` to pull our data from Project Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: HYPERPARAMETERS\n",
    "- Setting up our hyperparameters - we choose 8 heads of attention with a head size of 16\n",
    "- Our context size here is 32 tokens, and our embed dimension is 128, so this is a small network trained on one text file\n",
    "- We also choose 8 experts as our smaller feed forward networks within the model\n",
    "- We set top k to 2 to choose the top 2/8 experts for each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "n_embed = 128\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "head_size = 16\n",
    "dropout = 0.1\n",
    "context_size = 32\n",
    "num_experts = 8\n",
    "top_k = 2\n",
    "device = \"cpu\" # or \"cuda\"\n",
    "batch_size = 16\n",
    "eval_interval = 100\n",
    "eval_iters = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: DATA\n",
    "- Now we can collect our data - Moby Dick from Project Gutenberg\n",
    "- Download the text into a variable and print the first 1000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moby Dick text file from Project Gutenberg site\n",
    "resp = requests.get(\"https://www.gutenberg.org/cache/epub/2701/pg2701.txt\")\n",
    "\n",
    "# Tags for text filtering\n",
    "start = \"*** START OF THE PROJECT GUTENBERG EBOOK MOBY DICK; OR, THE WHALE ***\"\n",
    "end = \"*** END OF THE PROJECT GUTENBERG EBOOK MOBY DICK; OR, THE WHALE ***\"\n",
    "\n",
    "result = resp.text[resp.text.find(start):resp.text.find(end)]\n",
    "\n",
    "print(result[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: TOKENIZATION\n",
    "- Now we can set up our tokenization, which is the basic character level tokenizer we have been using in this module\n",
    "- We get the vocab size as the number of distinct characters in the data, as well as creating encoding and decoding functions to convert the data into tokens and back\n",
    "- Lastly, we encode the data and split into training and validation sets (90/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocab size\n",
    "chars = sorted(list(set(result)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)\n",
    "\n",
    "# Basic tokenizaiton as we've done before\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# Test the tokenizer\n",
    "print(encode(\"Hello test!\"))\n",
    "print(decode(encode(\"Hello test!\")))\n",
    "\n",
    "# Encode our dataset\n",
    "data = torch.tensor(encode(result), dtype=torch.long)\n",
    "\n",
    "# Train and validation split\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: UTIL METHODS\n",
    "- We need to create some util methods to use with our model when training\n",
    "- Namely we need a method to load a batch of data, and a method to estimate our loss\n",
    "- Both of these are not expressly needed for this code, just nice-to-have and makes things cleaner at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to get a batch (from Project 1)\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# Method to estimate loss (also from Project 1)\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: ATTENTION\n",
    "- We can now implement Multi Head Attention much like we have in the previous modules\n",
    "- We don't need to dive too far into this here, as we've done it before, and the implementation is the same\n",
    "- We create our AttentionHead module, and then combine heads in our MHA implementation\n",
    "- Nothing changes from previous implementations of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention implementation is re-used from the last few projects\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.k = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.q = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.v = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_size, context_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        key = self.k(x)\n",
    "        query = self.q(x)\n",
    "        weights = query @ key.transpose(-2,-1) * C**-0.5\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        value = self.v(x)\n",
    "        result = weights @ value\n",
    "        return result\n",
    "    \n",
    "# Same as above, MHA is copied exactly from Projects 1 and 2 - no need to change anything :-)\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7: MLP EXPERT AND ROUTER\n",
    "- Now is where the MoE-specific changes are needed.\n",
    "- We first create a Multi Layer Perceptron - our 'Expert' - that is the same as our usual MLP layer in classical Transformers\n",
    "- We then build the Top K Router, with k=2, to choose the top 2 experts to route tokens to. This is simply a linear projection from our embed dimensino to our # of experts\n",
    "- We use softmax and masking to round out the top 2 results to logits (summing to 1) and the remaining experts' values are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is where our FFN implementation changes, we rename it to our Expert module\n",
    "# It is still an MLP with ReLU activation, there are now num_experts of them\n",
    "class MLPExpert(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "    \n",
    "# Our Router or 'gating' module is used to determine which expert(s) receive a given token\n",
    "# Simply a linear projection from embedding dimension to num_experts\n",
    "# This is then softmaxed after setting non-topk values to negative infinity\n",
    "# The softmax returns sparse logits that sum to 1.0\n",
    "class TopkRouter(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(TopkRouter, self).__init__()\n",
    "        self.top_k = top_k\n",
    "        self.linear =nn.Linear(n_embed, num_experts)\n",
    "    \n",
    "    def forward(self, attention_output):\n",
    "        logits = self.linear(attention_output)\n",
    "        top_k_logits, indices = logits.topk(self.top_k, dim=-1)\n",
    "        zeros = torch.full_like(logits, float('-inf'))\n",
    "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
    "        router_output = F.softmax(sparse_logits, dim=-1)\n",
    "        return router_output, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8: SPARSE MOE\n",
    "- Now we can build our Sparse MoE Expert + Gating network module\n",
    "- This is the combination of everything we did in the last step, combining our router network with an MLPExpert module block for each expert\n",
    "- In the forward pass, we process each expert in parallel, and creating a mask for the inputs where the current expert is in top-k\n",
    "- If the mask contains values (we are in top-k) - process through the expert, apply gating scores, and sum the outputs across experts\n",
    "- Then, we return the final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now build our MoE module for our Transformer\n",
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(SparseMoE, self).__init__()\n",
    "        # Set up router, and make a list of num_experts MLP modules\n",
    "        self.router = TopkRouter(n_embed, num_experts, top_k)\n",
    "        self.experts = nn.ModuleList([MLPExpert(n_embed) for _ in range(num_experts)])\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        gating_output, indices = self.router(x)\n",
    "        final_output = torch.zeros_like(x)\n",
    "        flat_x = x.view(-1, x.size(-1))\n",
    "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
    "\n",
    "        # Iterate over experts\n",
    "        for i, expert in enumerate(self.experts):\n",
    "\n",
    "            expert_mask = (indices == i).any(dim=-1)\n",
    "            flat_mask = expert_mask.view(-1)\n",
    "\n",
    "            # If we have a logit, apply to expert and score\n",
    "            if flat_mask.any():\n",
    "                expert_input = flat_x[flat_mask]\n",
    "                expert_output = expert(expert_input)\n",
    "\n",
    "                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n",
    "                weighted_output = expert_output * gating_scores\n",
    "\n",
    "                # Sum the outputs and return\n",
    "                final_output[expert_mask] += weighted_output.squeeze(1)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 9: TRANSFORMER BLOCK\n",
    "- Finally, we can build our Transformer, replacing the classic MLP with the SpareMoE module we created above\n",
    "- Otherwise, the Transformer should be built and function the same way.\n",
    "- We pass our inputs through the MHA block, and they are then passed to the Router and Expert modules\n",
    "- Inputs are normalized before each module, and skip (residual) connections are used as well before returning the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer block like we've done before, but using our SparseMoE module instead of a FF layer\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_embed, n_head, num_experts, top_k):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.attention = MultiHeadAttention(n_head, head_size)\n",
    "        self.moe = SparseMoE(n_embed, num_experts, top_k)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        x = x + self.moe(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 10: LLM\n",
    "- We can build our Language Model using our Transformer block created above.\n",
    "- We set up our token and positional embeddings to process inputs\n",
    "- We also build our Transformer layers and add a final layer normalization before the LM head (linear projection from embed dimension to our vocab size)\n",
    "- The forward process is the same as we usually do with Transformer language models\n",
    "- We also add a generate method to run prediction / inference with the model without updating loss, simply by appending single token predictions in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we build our MoE language model\n",
    "class SparseMoELanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Token and positional embeddings\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(context_size, n_embed)\n",
    "\n",
    "        # num_layers layers of num_experts MLP modules each with n_head attention heads\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(n_embed, n_head=n_head, num_experts=num_experts, top_k=top_k) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed) # final layer norm\n",
    "\n",
    "        # LM head for output projection to vocab (token prediction)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Embed and sum\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # Pass through our building block layers\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        # Output of LM head is logits\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # Generation step if no targets\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    # Generate method to return new tokens up to max_new_tokens\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -context_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 11: TRAIN\n",
    "- For our training loop, we initialize our weights with [Xavier / Glorot initialization](https://365datascience.com/tutorials/machine-learning-tutorials/what-is-xavier-initialization/)\n",
    "- We set max iterations to 5000, and initialize our model, printing the # of parameters\n",
    "- We then create our AdamW optimizer and start our training loop\n",
    "- We print the loss values every 500 iterations\n",
    "- Update the weights and loop until we have completed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xavier initialization\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "\n",
    "max_iters = 5000\n",
    "    \n",
    "# Instantiate and initialize weights    \n",
    "model = SparseMoELanguageModel()\n",
    "model.apply(init_weights)\n",
    "\n",
    "# Get # of parameters\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # Every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 12: GENERATE\n",
    "- Now that training is done, we can use our generate method to display outputs\n",
    "- We initialize input as a 1x1 tensor of zero values, and pass it to the model to generate 2000 additional tokens\n",
    "- The model will print the result after it completes generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation step, print 2000 new tokens from empty 1x1 tensor\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
