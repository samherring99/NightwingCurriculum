{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 Project 2: T5\n",
    "\n",
    "Implement a [T5 model](https://blog.research.google/2020/02/exploring-transfer-learning-with-t5.html) using everything that has been covered in previous modules so far\n",
    "(Tokenization, Attention, Decoder layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: IMPORTS AND HYPERPARAMETERS\n",
    "- Outside of the usual `torch` imports, we need math for `sqrt` and requests to get out data\n",
    "- For our hyperparameters, we use a dropout of 0.1 and `batch_size` of 16\n",
    "- Our `chunk_size` here is used to split our batches into 'chunks' of size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import math\n",
    "\n",
    "dropout = 0.1\n",
    "chunk_size = 128\n",
    "batch_size = 32\n",
    "num_layers = 4\n",
    "embed_dim = 64\n",
    "num_heads = 4\n",
    "num_epochs = 500\n",
    "max_length = 1000\n",
    "\n",
    "eval_interval = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: LOAD THE DATA\n",
    "- We will use Project Gutenberg to load our data\n",
    "- I chose [Treasure Island](https://www.gutenberg.org/cache/epub/120) for this project\n",
    "- After splitting out the text of the book, print the first 1000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(\"https://www.gutenberg.org/cache/epub/120/pg120.txt\")\n",
    "\n",
    "start = \"*** START OF THE PROJECT GUTENBERG EBOOK TREASURE ISLAND ***\"\n",
    "end = \"*** END OF THE PROJECT GUTENBERG EBOOK TREASURE ISLAND ***\"\n",
    "\n",
    "result = resp.text[resp.text.find(start):resp.text.find(end)]\n",
    "\n",
    "print(result[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: TOKENIZATION\n",
    "- We will be re-implenting our Tokenizer from [last module's projects](https://github.com/samherring99/NightwingCurriculum/blob/main/module_1_nlp_basics/module_1_project_1.ipynb)\n",
    "- Starts with a base vocab of 256 tokens, performs BPE a set # of times (30) to create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.merges = {}\n",
    "        self.vocab = {}\n",
    "\n",
    "    def get_pair_counts(self, token_ids):\n",
    "        counts = {}\n",
    "        for pair in zip(token_ids, token_ids[1:]):\n",
    "            counts[pair] = counts.get(pair, 0) + 1\n",
    "        return counts\n",
    "\n",
    "    def new_token(self, token_ids, pair, index):\n",
    "        new_ids = []\n",
    "        i = 0\n",
    "        while i < len(token_ids):\n",
    "            if i < len(token_ids) - 1 and token_ids[i] == pair[0] and token_ids[i+1] == pair[1]:\n",
    "                new_ids.append(index)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_ids.append(token_ids[i])\n",
    "                i += 1\n",
    "        return new_ids\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        assert vocab_size >= 256\n",
    "        num_merges = vocab_size - 256\n",
    "\n",
    "        text_bytes = text.encode(\"utf-8\")\n",
    "        token_ids = list(text_bytes)\n",
    "\n",
    "        merges = {}\n",
    "        vocab = {index: bytes([index]) for index in range(256)}\n",
    "\n",
    "        for i in range(num_merges):\n",
    "            pair_counts = self.get_pair_counts(token_ids)\n",
    "            pair = max(pair_counts, key=pair_counts.get)\n",
    "            index = 256 + i\n",
    "            token_ids = self.new_token(token_ids, pair, index)\n",
    "            merges[pair] = index\n",
    "            vocab[index] = vocab[pair[0]] + vocab[pair[1]]\n",
    "        \n",
    "        self.merges = merges\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        while len(tokens) >= 2:\n",
    "            stats = self.get_pair_counts(tokens)\n",
    "            pair = min(stats, key= lambda x: self.merges.get(x, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            index = self.merges[pair]\n",
    "            tokens = self.new_token(tokens, pair, index)\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        tokens = b\"\".join(self.vocab[index] for index in token_ids)\n",
    "        text = tokens.decode(\"utf-8\", errors='replace')\n",
    "        return text\n",
    "\n",
    "\n",
    "token = Tokenizer()\n",
    "token.train(result, 486) # We want to do 230 (256 is base vocab size) merges, as an example\n",
    "\n",
    "vocab = token.vocab\n",
    "\n",
    "print(token.merges)\n",
    "print(token.decode(token.encode(\"Hello! this is a text string!\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: SELF-ATTENTION\n",
    "- We will be re-implementing our MultiHeadedAttention class from the last project in this module\n",
    "- The MHA implementation remains the same as when we did it last, nothing has changed\n",
    "- See [Module 2 Project 1]() for an in depth explanation on attention and multi-headed attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(chunk_size, chunk_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        weights = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        weights = F.softmax(weights, dim=1)\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        v = self.value(x)\n",
    "        result = weights @ v\n",
    "\n",
    "        return result\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(head_size) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        result = self.dropout(self.projection(result))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: CROSS-ATTENTION\n",
    "- Here, we need to implement cross attention. The difference being that we 'attend' to the outputs from our Encoder layer in T5, we are no longer 'self-attending' across the same input for Q, K, and V\n",
    "- Now, our Q remains the same, but our K and V values come from the outputs of the Encoder layer (more on this below)\n",
    "- Attention across these inputs is calculated in a similar way, but we multiply our inputs for our Linear layers by our `num_heads` to 'cross-attend' over all of our attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedCrossAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = embed_dim // num_heads\n",
    "        \n",
    "        self.query = nn.Linear(4* embed_dim, embed_dim, bias=False)\n",
    "        self.key = nn.Linear(4*embed_dim, embed_dim, bias=False)\n",
    "        self.value = nn.Linear(4*embed_dim, embed_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_projection = nn.Linear(embed_dim, 4*embed_dim)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        B, T_q, _ = query.size()\n",
    "        _, T_k, _ = key.size()\n",
    "\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)\n",
    "        value = self.value(value)\n",
    "        \n",
    "        query = query.view(B, T_q, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        key = key.view(B, T_k, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        value = value.view(B, T_k, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_size)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context = torch.matmul(attn_weights, value)\n",
    "        context = context.transpose(1, 2).contiguous().view(B, T_q, -1)\n",
    "        output = self.out_projection(context)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: FEED FORWARD\n",
    "- Here, our Feed Forward network is implemented in much the same way as the last project on Transformers\n",
    "- A linear layer projects our embedding to 4x, performs ReLU activation, and reduces the embedding dimension back to `embed_dim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4*embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*embed_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7: ENCODER LAYER\n",
    "- Our encoder layer is written much the same as our `BuildingBlocks` in the Transformers project previously\n",
    "- We have our MHA implementation, our Feed Forward network, and 2 layer normalizations, each one performed before MHA and Feed Forward respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        head_size = embed_dim // num_heads\n",
    "        self.attention = MultiHeadedAttention(num_heads, head_size)\n",
    "        self.feed_forward = FeedForward(embed_dim)\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.attention(self.layer_norm1(x))\n",
    "        x = x + self.feed_forward(self.layer_norm2(x))\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8: DECODER LAYER\n",
    "- Here, our decoder layer needs to 'cross-attend' to our encoder layer's outputs\n",
    "- It's written out very much the same way, but with `CrossAttention` instead of our usual MHA\n",
    "- The `x + ` operator below performes our residual connection to help deal with the vanishing gradient problem\n",
    "- Here, layer normalization is performed after cross-attention and our feed forward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        head_size = embed_dim // num_heads\n",
    "        self.attention = MultiHeadedCrossAttention(num_heads, head_size)\n",
    "        self.feed_forward = FeedForward(embed_dim)\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, encoder_output, mask=None):\n",
    "        cross_attention_output = self.attention(self.layer_norm1(x), encoder_output, encoder_output, mask)\n",
    "        x = x + cross_attention_output\n",
    "        ff_output = self.feed_forward(self.layer_norm2(x))\n",
    "        x = x + ff_output\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 9: T5 MODEL\n",
    "- Now we can put everything together\n",
    "- Using `num_layers` we build out our Encoder and Decoder blocks\n",
    "- We cap these off with a linear projection to our `vocab_size` to get logits for the next token\n",
    "- Using the `forward` call in our decoder layer with both `input_embed` and our `target_embed`, we get cross-attention results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_layers, num_heads):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  \n",
    "        self.layer_norm_f = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(embed_dim, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(embed_dim, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, target_ids):\n",
    "        self.input_embed = self.embedding(input_ids)\n",
    "        self.target_embed = self.embedding(target_ids)\n",
    "\n",
    "        for layer in self.encoder_layers:\n",
    "            self.input_embed = layer(self.input_embed)\n",
    "        \n",
    "        for layer in self.decoder_layers:\n",
    "            self.target_embed = layer(self.target_embed, self.input_embed)\n",
    "\n",
    "        norm_result = self.layer_norm_f(self.target_embed)\n",
    "\n",
    "        output = self.fc(norm_result)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 10: TRAINING LOOP\n",
    "- Here we implement a common training loop using CrossEntropyLoss and Adam for our optimizer\n",
    "- We set a learning rate of 0.001 arbitrarily\n",
    "- We set our initialization parameters for our model to be 4 layers, 4 heads in each layer, with an `embed_dim` of 32\n",
    "- We want to train for 100 epochs and to generate up to 1000 characters as a test output when training completes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "model = T5(vocab_size, embed_dim, num_layers, num_heads)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "data = torch.tensor(token.encode(result), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    data = train_data\n",
    "    index = torch.randint(len(data) - chunk_size, (batch_size,))\n",
    "    input_ids = torch.stack([data[i:i+chunk_size] for i in index])\n",
    "    target_ids = torch.stack([data[i+1:i+chunk_size+1] for i in index])\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(input_ids, target_ids)\n",
    "    loss = criterion(output.view(-1, vocab_size), target_ids.view(-1))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "    \n",
    "    if epoch % eval_interval == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(input_ids)}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        data = val_data\n",
    "        index = torch.randint(len(data) - chunk_size, (batch_size,))\n",
    "        val_input_ids = torch.stack([data[i:i+chunk_size] for i in index])\n",
    "        val_target_ids = torch.stack([data[i+1:i+chunk_size+1] for i in index])\n",
    "        val_output = model(val_input_ids, val_target_ids)\n",
    "        val_loss += criterion(val_output.view(-1, vocab_size), val_target_ids.view(-1)).item()\n",
    "    #print(f\"Validation Loss: {val_loss / len(val_input_ids)}\")\n",
    "\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 11: GENERATE TEXT\n",
    "- Here we evaluate our trained model to generate sample text\n",
    "- This should be nonsense, just confirming our model works for generating text from token sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    input_ids = torch.tensor(token.encode(\"The \")).unsqueeze(0)\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        context = input_ids[:, -chunk_size:]\n",
    "        logits = model(context, context)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_index = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        input_ids = torch.cat((input_ids, next_index), dim=1)\n",
    "\n",
    "    print(token.decode(input_ids[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
