{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 Project 1: TRANSFORMERS\n",
    "\n",
    "Implement a basic version of a Transformer based language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: IMPORTS AND HYPERPARAMETERS\n",
    "- Import necessary libraries\n",
    "- Set up our hyperparameters\n",
    "- `embed_dim` / 4 needs to be factorizable by 8\n",
    "- `chunk_size` is our context length\n",
    "- 4 layers of the network, and 4 attention heads per layer\n",
    "- `batch_size` is the size of our batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "## Hyperparameters\n",
    "chunk_size = 128\n",
    "batch_size = 16\n",
    "embed_dim = 32\n",
    "num_layers = 4\n",
    "num_heads = 4\n",
    "\n",
    "learning_rate = 3e-4\n",
    "dropout = 0.2\n",
    "\n",
    "eval_iters = 200\n",
    "eval_interval = 200\n",
    "epochs = 5000\n",
    "\n",
    "max_new_tokens = 400\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: DATASET COLLECTION\n",
    "- We chose Winnie the Pooh eBook from Project Gutenberg for sample data\n",
    "- Select our start and end tags of the text we want, and extract it into `result`\n",
    "- Create a simple vocabulary by getting all possible characters in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(\"https://www.gutenberg.org/cache/epub/67098/pg67098.txt\")\n",
    "\n",
    "# Tags for text filtering\n",
    "start = \"*** START OF THE PROJECT GUTENBERG EBOOK WINNIE-THE-POOH ***\"\n",
    "end = \"*** END OF THE PROJECT GUTENBERG EBOOK WINNIE-THE-POOH ***\"\n",
    "\n",
    "result = resp.text[resp.text.find(start):resp.text.find(end)]\n",
    "\n",
    "print(result[:1000])\n",
    "\n",
    "chars = sorted(list(set(result)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: BASIC TOKENIZATION\n",
    "- Here we will be doing the most basic form of tokenization - character level\n",
    "- No BPE algorithm here (yet)\n",
    "- Just `encode` and `decode` functions for use with our text data like classic tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: TRAIN AND VALIDATION SPLIT\n",
    "- Split the data between train and validation sets (90% train, 10% val)\n",
    "- Print an example 'chunk' in the training data\n",
    "- We will define a method to get a random batch from our data of size `chunk_size`\n",
    "- We also get the set of 'next' tokens for a given chunk, i.e. index_of_chunk_start shifted over by 1\n",
    "- Now we go through a batch, and all chunks within the batch, and print the `context` and `target`\n",
    "- Here, `context` is the current word and `target` is the next word to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(result), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])\n",
    "\n",
    "n = int(0.9*len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "x = train_data[:chunk_size]\n",
    "y = train_data[1:chunk_size+1]\n",
    "\n",
    "for t in range(chunk_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(str(context) + \" \" + str(target))\n",
    "\n",
    "#torch.manual_seed(1337)\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    index = torch.randint(len(data) - chunk_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+chunk_size] for i in index])\n",
    "    y = torch.stack([data[i+1:i+chunk_size+1] for i in index])\n",
    "    return x, y\n",
    "\n",
    "x_batch, y_batch = get_batch(\"train\")\n",
    "\n",
    "print(\"Inputs\")\n",
    "print(x_batch.shape)\n",
    "print(x_batch)\n",
    "print(\"Targets\")\n",
    "print(y_batch.shape)\n",
    "print(y_batch)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(chunk_size):\n",
    "        context = x_batch[b, :t+1]\n",
    "        target = y_batch[b, t]\n",
    "        print(str(context.tolist()) + \" \" + str(target.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: ATTENTION\n",
    "- Here, we will create our simple self-attention head, much like we did in the BERT implementation\n",
    "- We assign linear transformations to our key, query, and value vectors, as well as a 'triangulation' buffer that will zero out all entries in our attention weight matrix that correspond to 'future' probabilities (we only care about the current word and all words that came before it when predicting the next token, not future tokens, as that would give us the answer before calulating it)\n",
    "- Our weight matrix is formed by multiplying Q and K.T, which gives us a positionally encoded weight matrix that details the relevance of each token in a sentence to all other tokens in the sentence.\n",
    "- Once this is done, we apply trianglulation and a softmax to normalize the values between [0, 1]\n",
    "- Finally, we can multiply this normalized weight matrix by our value vector (the tokens in the input sentence) to get a token-wise weighted distribution of 'relevance', or attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(chunk_size, chunk_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        weights = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        weights = F.softmax(weights, dim=1)\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        v = self.value(x)\n",
    "        result = weights @ v\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: MULTI HEADED ATTENTION\n",
    "- Using what we did above, we can combine our Attention Heads into a `nn.ModuleList` layer, the size of which is `num_heads`\n",
    "- Afterwards, we add a linear transformation layer and dropout to go from our multi headed attention to a projection of the probabilities per-token in our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(head_size) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        result = self.dropout(self.projection(result))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: FEED FORWARD NETWORK\n",
    "- The last piece we really need to get this all running is the Feed Forward network\n",
    "- This is just a sequence of a linear transformation, ReLU activation, and another linear transformation, with the usual dropout applied at the end\n",
    "- The `embed_dim` parameter is scaled up by 4x in the intermediate layer as in the original Transformer architecture paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4*embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*embed_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7: BUILDING BLOCKS\n",
    "- Now we can put everything together in our layers, called Building Blocks\n",
    "- We calculate the `head_size` based on hyperparameters\n",
    "- We will run each layer with the following order:\n",
    "    - Multi Headed Attention\n",
    "    - LayerNorm (row-wise normalization)\n",
    "    - Feed Forward network\n",
    "    - Another LayerNorm\n",
    "- We will then return the output (of size `embed_dim`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildingBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        head_size = embed_dim // num_heads\n",
    "        self.self_attention = MultiHeadedAttention(num_heads, head_size)\n",
    "        self.feed_forward = FeedForward(embed_dim)\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.self_attention(self.layer_norm1(x))\n",
    "        x = x + self.feed_forward(self.layer_norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8: SIMPLE TRANSFORMER\n",
    "- Using our Building Blocks above, we have the components to build our Simple Transformer language model\n",
    "- We first create a token embedding table and a position embedding table\n",
    "- The token embedding input is of size `vocab_size`, and the position embedding input is of size `chunk_size`\n",
    "- We build a sequential layer of our Building Blocks accoding to `num_layers`\n",
    "- We add a final LayerNorm to normalize the output, before one final linear transformation layer as the LM head (the output size here is `vocab_size` - which in turn predicts the likeliest 'next' character based on our attention weight matrix)\n",
    "- We first embed the input in our token embedding, and we then embed our positions in the form of the 'time' dimension of our input (basically just index of token in the string)\n",
    "- After adding them together to get an embedding of both our tokens and their positional relationships, we pass the input to our Building Block layers\n",
    "- Once the output is returned, we do our layer normalization, and the final linear transformation to size `vocab_size` to get our `logits`, which we can use for predicting the next token\n",
    "- Our model also has an `estimate_loss` method to average the loss over a number of iterations for better logging during training (not expressly necessary, just nice to have)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding_table = nn.Embedding(chunk_size, embed_dim)\n",
    "        self.blocks = nn.Sequential(*[BuildingBlock(embed_dim, num_heads=num_heads) for _ in range(num_layers)])\n",
    "        self.layer_norm_f = nn.LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "\n",
    "        B, T = index.shape\n",
    "\n",
    "        token_embedding = self.token_embedding_table(index)\n",
    "        position_embedding = self.position_embedding_table(torch.arange(T, device='cpu'))\n",
    "        x = token_embedding + position_embedding\n",
    "        x = self.blocks(x)\n",
    "        x = self.layer_norm_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # Generation step if we have no target\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            context = index[:, -chunk_size:]\n",
    "            logits, loss = self(context)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_index = torch.multinomial(probs, num_samples=1)\n",
    "            index = torch.cat((index, next_index), dim=1)\n",
    "\n",
    "        return index\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 9: BASIC GENERATION\n",
    "- Now that our Simple Transformer model is created, we can text generation on an un-trained model\n",
    "- We first demonstrate an example of doing a 'forward' pass with the model, and observing the loss and logits returned, to ensure everything is functioning correctly\n",
    "- Generation works by simply calling the model's `generate` function with an input (random in this case), which returns the logits, which are then softmaxed and used with `torch.multinomial` to generate the prediction for the 'next' token, until `max_new_tokens` have been generated\n",
    "- This prediction is then decoded after all tokens have been generated, this is how we generate text with a language model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleTransformer(embed_dim)\n",
    "logits, loss = model(x_batch, y_batch)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(model.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 10: MODEL TRAINING\n",
    "- Now it is time to set up our training loop for our model\n",
    "- We use AdamW as our optimizer here (as with the original paper)\n",
    "- Every `eval_interval` steps, we estimate the loss over the interval and display - just nice to have\n",
    "- We get a random batch from our data, and run it through our Simple Transformer model\n",
    "- We use `cross_entropy` to calculate the loss, and step through the optimizer and backpropagate\n",
    "- After we have reached our desired number of epochs, we output the final loss and a sample generation\n",
    "- We will have successfully trained a Simple Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for step in range(epochs):\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    x_batch, y_batch = get_batch('train')\n",
    "\n",
    "    logits, loss = model(x_batch, y_batch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n",
    "print(decode(model.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=max_new_tokens)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
