{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 Project 3 Part 1: Finetuning\n",
    "\n",
    "Finetune a model using LoRA on some sample data using llama.cpp and confirm it worked by displaying some of output.\n",
    "Perform unsupervised finetuning with this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: GET THE MODEL\n",
    "- Get the model from HuggingFace\n",
    "- I choce Wizard-Vicuna-13B-uncensored-SuperHOT-8K for this project\n",
    "- You can use whetever model you want for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "wget https://huggingface.co/JohanAR/Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GGUF/resolve/main/wizard-vicuna-13b-uncensored-superhot-8k.q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: GET THE DATA\n",
    "- We want to download some sample data to finetune on\n",
    "- I randomly picked a book from Project Gutenberg for this, ideally we'd want much more data\n",
    "- Just making sure the finetuning process works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "wget https://www.gutenberg.org/cache/epub/4300/pg4300.txt -O data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: FINETUNE WITH LLAMA.CPP\n",
    "- Run the finetuning script from llama.cpp \n",
    "- Note the checkpoint outputs and `lora-out` parameter\n",
    "- We use a batch size of 4 and a context window of 64 with 6 threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "./finetune \\\n",
    "        --model-base models/wizard-vicuna-13b-uncensored-superhot-8k.q5_K_M.gguf \\\n",
    "        --checkpoint-in  chk-lora-wizard-vicuna-13b-uncensored-superhot-8k.q5_K_M.gguf \\\n",
    "        --checkpoint-out chk-lora-wizard-vicuna-13b-uncensored-superhot-8k.q5_K_M-ITERATION.gguf \\\n",
    "        --lora-out lora-wizard-vicuna-13b-uncensored-superhot-8k.q5_K_M-ITERATION.bin \\\n",
    "        --train-data \"data.txt\" \\\n",
    "        --save-every 10 \\\n",
    "        --threads 6 --adam-iter 30 --batch 4 --ctx 64 \\\n",
    "        --use-checkpointing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: TEST THE FINETUNED MODEL\n",
    "- After finetuning, display some output of the model\n",
    "- The below code is used for Mac (M1 is tested)\n",
    "- Context window of 4096 is used here to max out RAM on the M1 chip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 4096  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "language_model = LlamaCpp(\n",
    "    model_path=\"full_path_to_model/wizard-vicuna-13b-uncensored-superhot-8k.q4_K_M.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=4096,\n",
    "    f16_kv=True, \n",
    "    callback_manager=callback_manager,\n",
    "    verbose=False,\n",
    "    echo=False\n",
    ")\n",
    "\n",
    "result = language_model(\"What is the meaning of life?\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
