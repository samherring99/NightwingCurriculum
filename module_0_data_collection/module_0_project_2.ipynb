{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 0 Project 2: Data Collection\n",
    "\n",
    "- Implement a data preprocessing pipeline from [sklearn](https://scikit-learn.org/stable/)\n",
    "- Use advanced preprocessing techniques with data augmentation on a sample dataset\n",
    "- The goal of this module is to be able to set up any type of preprocessing pipeline based on the needs of your dataset and end goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: IMPORTS AND DATASET GENERATION\n",
    "- Import necessary libraries - `torch`, `numpy`, and of course `sklearn`\n",
    "- Generate a random dataset for ease of use including categorical values and null values - we will be using random numbers for our data in this module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Randomly generated sample data\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'numerical_1': np.random.randint(1, 100, 100),\n",
    "    'numerical_2': np.random.normal(1000, 100, 100),\n",
    "    'numerical_3': np.random.choice([np.nan, 5, 10, 15], 100),\n",
    "    'categorical': np.random.choice(['A', 'B', 'C'], 100),\n",
    "    'target': np.random.randint(0, 2, 100)\n",
    "}\n",
    "\n",
    "# Put the data into a DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: DEFINE FEATURES AND SPLIT DATA\n",
    "- Define the feature types and split up the data into train and test sets\n",
    "- We want to do this before any preprocessing steps\n",
    "- We have 3 nuerical features and 1 categorical feature (shown above)\n",
    "- We set our 'target' to be the `y` value here - or the 'answer' we want to find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical and categorical features\n",
    "numerical_features = ['numerical_1', 'numerical_2', 'numerical_3']\n",
    "categorical_features = ['categorical']\n",
    "\n",
    "# Splitting data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: DEFINE SKLEARN PIPELINE\n",
    "- Define the steps for the preprocessing pipeline setup\n",
    "- Handle categorical values with one hot encoding, null values, scaling and standardization/normalization\n",
    "- Add a piece for polynomial feature augmentation to increase size and diversity in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for numerical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False))\n",
    "])\n",
    "\n",
    "# Pipeline for categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('encoder', OneHotEncoder())\n",
    "])\n",
    "\n",
    "# Combine numerical and categorical transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define the final pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: FIT THE PIPELINE, DISPLAY AND SAVE THE DATA\n",
    "- Fit the pipeline to our generated dataset (pass the data through the pipeline and capture the result)\n",
    "- Display the preprocessed and augmented data and save it to a CSV file\n",
    "- We now have a fully prepared dataset for machine learning use cases, and can repeat these steps for any future task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the training data\n",
    "X_train_augmented = pipeline.fit_transform(X_train, y_train)\n",
    "print(X_train_augmented)\n",
    "\n",
    "# Save data to CSV format\n",
    "np.savetxt(\"data.csv\", X_train_augmented, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
