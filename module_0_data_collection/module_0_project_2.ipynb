{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 0 Project 2: Data Collection\n",
    "\n",
    "Implement a data preprocessing pipeline from [sklearn](https://scikit-learn.org/stable/), use advanced preprocessing techniques with data augmentation on a sample dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: IMPORTS AND DATASET GENERATION\n",
    "- Import necessary libraries\n",
    "- Generate a random dataset for ease of use including categorical values and null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Randomly generated sample data\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'numerical_1': np.random.randint(1, 100, 100),\n",
    "    'numerical_2': np.random.normal(1000, 100, 100),\n",
    "    'numerical_3': np.random.choice([np.nan, 5, 10, 15], 100),\n",
    "    'categorical': np.random.choice(['A', 'B', 'C'], 100),\n",
    "    'target': np.random.randint(0, 2, 100)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: DEFINE FEATURES AND SPLIT DATA\n",
    "- Define the feature types and split up the data into train and test sets\n",
    "- We want to do this before any preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical and categorical features\n",
    "numerical_features = ['numerical_1', 'numerical_2', 'numerical_3']\n",
    "categorical_features = ['categorical']\n",
    "\n",
    "# Splitting data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: DEFINE SKLEARN PIPELINE\n",
    "- Define the pipeline steps for the preprocessing pipeline setup\n",
    "- Handle categorical values, null values, scaling and standardization/normalization\n",
    "- Add a piece for polynomial feature augmentation to increase diversity in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for numerical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False))\n",
    "])\n",
    "\n",
    "# Pipeline for categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('encoder', OneHotEncoder())\n",
    "])\n",
    "\n",
    "# Combine numerical and categorical transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define the final pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: FIT THE PIPELINE, DISPLAY AND SAVE THE DATA\n",
    "- Fit the pipeline to our generated dataset\n",
    "- Display the preprocessed and augmented data and save it to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the training data\n",
    "X_train_augmented = pipeline.fit_transform(X_train, y_train)\n",
    "print(X_train_augmented)\n",
    "\n",
    "# Save data to CSV format\n",
    "np.savetxt(\"data.csv\", X_train_augmented, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
