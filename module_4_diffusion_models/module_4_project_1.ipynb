{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4 Project 1: CLIP\n",
    "- Implement a simple [CLIP](https://openai.com/research/clip) model to start working with image-text embeddings\n",
    "- Train the model on some data and evlauate the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: IMPORTS\n",
    "- We need to import `torch` and `transformers` for most of the utility methods needed here, as well as our tokenizer and dataloader classes\n",
    "- We want `matplotlib` and `numpy` for displaying the images after training when queried with a caption string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: HYPERPARAMETERS\n",
    "- We will be using [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) for our language model in this example\n",
    "- We will be using a batch size of 128 and a max sequence length of 32 for 3 epochs\n",
    "- Embed dimension is 512 for this model, and the standard 768 for our transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size=128\n",
    "text_model='distilbert-base-multilingual-cased' # Using a simple text model here\n",
    "transformer_embed_dim=768\n",
    "embed_dim=512\n",
    "max_len=32\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: DATASET\n",
    "- Now we can get our dataset to train the model\n",
    "- We will be using the [Flickr30kDataset](https://datasets.activeloop.ai/docs/ml/datasets/flickr30k-dataset/) and resizing images to 224 x 224px\n",
    "- Data is a collection of images that depict a wide range of activites with descriptive captions, good for benchmarking this task (sentence descriptions of images)\n",
    "- Finally, we wrap our dataset object with a torch DataLoader to have easy batched training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper class for our dataset - Flickr30k\n",
    "class Flickr30kDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.dataset = load_dataset(\"nlphuji/flickr30k\", cache_dir=\"./huggingface_data\")\n",
    "\n",
    "        # We resize images to 224 x 224\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        self.cap_per_image = 2\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_rows[\"test\"] * self.cap_per_image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        original_idx = idx // self.cap_per_image\n",
    "\n",
    "        image = self.dataset[\"test\"][original_idx][\"image\"].convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        caption = self.dataset[\"test\"][original_idx][\"caption\"][idx % self.cap_per_image]\n",
    "\n",
    "        return {\"image\": image, \"caption\": caption}\n",
    "\n",
    "# Dataset and DataLoader for batched retrieval\n",
    "flickr30k_custom_dataset = Flickr30kDataset()\n",
    "clip_dataloader = DataLoader(flickr30k_custom_dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: PROJECTION LAYER\n",
    "- Here we create our projection layer, which is used to project the text and image encoders results into size `embed_dim`\n",
    "- Once this is done, we will have text and images existing in the same high dimensional space, and we can use this knowledge to compare and contrast them based on semantic meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP uses a projection layer to cast text and images into the same dimensions\n",
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_in: int, d_out: int, p: float=0.5) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Two linear projections followed by layernorm and dropout\n",
    "        self.linear1 = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.linear2 = nn.Linear(d_out, d_out, bias=False)\n",
    "        self.layer_norm = nn.LayerNorm(d_out)\n",
    "        self.drop = nn.Dropout(p)\n",
    "\n",
    "    # We use GeLU activation here as well\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        embed1 = self.linear1(x)\n",
    "        embed2 = self.drop(self.linear2(F.gelu(embed1)))\n",
    "        embeds = self.layer_norm(embed1 + embed2)\n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: IMAGE ENCODER\n",
    "- Now that our projection layer is done above, we can buld the image encoder layer\n",
    "- This layer takes in an image and returns the projected and normalized form in high dimensional vector space\n",
    "- Our base model here is [ResNest34](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet34.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module to encode our images using ResNet34\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, d_out: int) -> None:\n",
    "        super().__init__()\n",
    "        base = models.resnet34(pretrained=True)\n",
    "        d_in = base.fc.in_features\n",
    "\n",
    "        # Set the fully connected layer to be the identity function - we want our image embeddings returned\n",
    "        base.fc = nn.Identity()\n",
    "        self.base = base\n",
    "\n",
    "        # Projection layer is used here\n",
    "        self.projection = ProjectionLayer(d_in, d_out)\n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    # Return after the image model embedding pass and the projection layer\n",
    "    def forward(self, x):\n",
    "        projected_vec = self.projection(self.base(x))\n",
    "        projection_len = torch.norm(projected_vec, dim=-1, keepdim=True)\n",
    "        return projected_vec / projection_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: TEXT ENCODER\n",
    "- Now, we create our text encoder layer similar to the image encoder above\n",
    "- We take the results of the base model, project to `embed_dim` dimensions, and return\n",
    "- These two method return similarly formatted vectors of the same size for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module for encoding text into high dimensional space with our images, done above\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, d_out: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Instantiate our text model\n",
    "        self.base = AutoModel.from_pretrained(text_model)\n",
    "\n",
    "        # And our projection layer\n",
    "        self.projection = ProjectionLayer(transformer_embed_dim, d_out)\n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    # Forward pass returns our text embeddings and projection layer output\n",
    "    def forward(self, x):\n",
    "        out = self.base(x)[0]\n",
    "        out = out[:, 0, :]\n",
    "        projected_vec = self.projection(out)\n",
    "        projection_len = torch.norm(projected_vec, dim=-1, keepdim=True)\n",
    "        return projected_vec / projection_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7: LOSS AND SIMILARITY\n",
    "- We can set our loss function and similarity metrics now\n",
    "- We will be doing cross entropy loss between axes with our logits/labels\n",
    "- This allows us to get both a comparing (similar) and contrasting (dissimilar) measure of accuracy in this space\n",
    "- By averaging these two measures, we get a consistent measure of vector similarity while retaining semantic comparison\n",
    "- Our `metrics` method words by comparing the image and caption candidates elementwise and returning the highest accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define a method to calculate our loss\n",
    "def CLIP_loss(logits: torch.Tensor) -> torch.Tensor:\n",
    "    n = logits.shape[1]      # number of samples\n",
    "    labels = torch.arange(n) # Create labels tensor\n",
    "    # Calculate cross entropy losses along axis 0 and 1\n",
    "    loss_i = F.cross_entropy(logits.transpose(0, 1), labels, reduction=\"mean\")\n",
    "    loss_t = F.cross_entropy(logits, labels, reduction=\"mean\")\n",
    "    # Calculate the final loss\n",
    "    loss = (loss_i + loss_t) / 2\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Method to calculate our similarity scores\n",
    "def metrics(similarity: torch.Tensor):\n",
    "    y = torch.arange(len(similarity)).to(similarity.device)\n",
    "    img2cap_match_idx = similarity.argmax(dim=1)\n",
    "    cap2img_match_idx = similarity.argmax(dim=0)\n",
    "\n",
    "    img_acc = (img2cap_match_idx == y).float().mean()\n",
    "    cap_acc = (cap2img_match_idx == y).float().mean()\n",
    "\n",
    "    return img_acc, cap_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8: TOKENIZER\n",
    "- We now can build a wrapper for our tokenizer\n",
    "- This step is not explicitly necessary, it just helps abstract tokenizer operations out from our main code, as we don't need to focus too much on it in this module\n",
    "- This class wraps the initialization and calling of the tokenizer with appropriate parameters\n",
    "- We will be using the accompanying tokenizer for DistillBeRT as well here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic wrapper class for our tokenizer - makes life easier\n",
    "class Tokenizer:\n",
    "    def __init__(self, tokenizer: BertTokenizer) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, x: str) -> AutoTokenizer:\n",
    "        return self.tokenizer(\n",
    "            x, max_length=max_len, truncation=True, padding=True, return_tensors=\"pt\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 9: BUILD CLIP\n",
    "- Once eveyerhting above has been completed, we can complete our CLIP implementation\n",
    "- We initialize our image and text encoders with `embed_dim` as the output dimension\n",
    "- We create our wrapped tokenizer class and set our learning rate to 1e-3\n",
    "- For the `forward` call, we tokenize the text, embed our image with the encoder, embed the caption text with our text encoder, and then multiply them to get our similarity matrix.\n",
    "- This matrix is then passed into `CLIP_loss` and `metrics` for similarity scoring, we return the loss and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we build our CLIP model\n",
    "class NightwingCLIP(nn.Module):\n",
    "    def __init__(self, lr: float = 1e-3) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Set up text and image encoders\n",
    "        self.image_encoder = ImageEncoder(embed_dim)\n",
    "        self.text_encoder = TextEncoder(embed_dim)\n",
    "\n",
    "        # Set tokenizer\n",
    "        self.tokenizer = Tokenizer(AutoTokenizer.from_pretrained(text_model))\n",
    "\n",
    "        self.lr = lr\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    def forward(self, images, text):\n",
    "        # Tokenize text\n",
    "        text = self.tokenizer(text).to(self.device)\n",
    "\n",
    "        # Embed image and captions\n",
    "        image_embed = self.image_encoder(images)\n",
    "        caption_embed = self.text_encoder(text[\"input_ids\"])\n",
    "\n",
    "        # Similarity matrix is Caption @ Image.T\n",
    "        similarity = caption_embed @ image_embed.T\n",
    "\n",
    "        # Calculate our loss and accuracy from the above matrix\n",
    "        loss = CLIP_loss(similarity)\n",
    "        img_acc, cap_acc = metrics(similarity)\n",
    "\n",
    "        # Return the loss and scores\n",
    "        return loss, img_acc, cap_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 10: TRAINING\n",
    "- For our training loop, we initialize our model and an [Adam](https://arxiv.org/abs/1412.6980) optimizer\n",
    "- We then iterate through epochs, loading data and updating weights based on model's performance over the training set\n",
    "- When training is complete we will have a simple, trained version of CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our model\n",
    "model = NightwingCLIP().to('cuda')\n",
    "\n",
    "# Set our optimizer to Adam\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.text_encoder.parameters()},\n",
    "    {'params': model.image_encoder.parameters()}\n",
    "], lr=model.lr)\n",
    "\n",
    "batch_zero = True\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(0, num_epochs):\n",
    "    model.train()\n",
    "    for batch in clip_dataloader:\n",
    "        image = batch[\"image\"].to('cuda')\n",
    "        text = batch[\"caption\"]\n",
    "        loss, img_acc, cap_acc = model(image, text)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_zero:\n",
    "          print(f\"Epoch [{0}/{num_epochs}], Batch Loss: {loss.item()}\")\n",
    "          batch_zero = False\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Batch Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 11: DISPLAY MATCHING IMAGES\n",
    "- We need a method to turn our images from numbers in matrices to displaying on a screen\n",
    "- We also want to provide a query string and be able to receive matching images back as a demonstration\n",
    "- We can do this by tokenizing the query string, embedding it into text, and matching similarity to our dataset of images in embedded form\n",
    "- Once the top indices are determined for similarity, we select and display them using `matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to display images in the data that match a given query - we want up to 4\n",
    "def display_matching_images(model, query_string, flickr30k_dataset, clip_dataloader, top_k=4):\n",
    "    tokenized_query = model.tokenizer(query_string)\n",
    "\n",
    "    # Embed query string\n",
    "    with torch.no_grad():\n",
    "        query_embedding = model.text_encoder(tokenized_query[\"input_ids\"].to(model.device))\n",
    "\n",
    "    # Embed all images\n",
    "    image_embeddings = []\n",
    "    for batch in clip_dataloader:\n",
    "        images = batch[\"image\"].to(model.device)\n",
    "        with torch.no_grad():\n",
    "            image_embedding = model.image_encoder(images)\n",
    "        image_embeddings.append(image_embedding)\n",
    "    image_embeddings = torch.cat(image_embeddings)\n",
    "\n",
    "    # Simiarity from query to all images\n",
    "    with torch.no_grad():\n",
    "        similarity_scores = torch.matmul(query_embedding, image_embeddings.T)\n",
    "\n",
    "    # Get top matches\n",
    "    top_indices = similarity_scores.squeeze().argsort(dim=-1, descending=True)[:top_k]\n",
    "\n",
    "    # Plot (display) the top 4 matching images to the query string\n",
    "    fig, axes = plt.subplots(1, top_k, figsize=(15, 5))\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        image = flickr30k_dataset[idx.item()][\"image\"]\n",
    "        image_np = np.transpose(image.cpu().numpy(), (1, 2, 0))\n",
    "        axes[i].imshow(image_np)\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 12: DEMO\n",
    "- We can now run a simple demo on our CLIP implementation\n",
    "- We pass in a query string of images we want to see, and run it\n",
    "- The model will output similar images matching the text to the best of its ability, and we are done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo a query example\n",
    "query = \"A cat sitting outside\"\n",
    "display_matching_images(model, query, flickr30k_custom_dataset, clip_dataloader)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
