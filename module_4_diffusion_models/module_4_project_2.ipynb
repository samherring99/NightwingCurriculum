{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4 Project 2: Stable Diffusion\n",
    "- Implement and train a simple version of [Stable Diffusion](https://en.wikipedia.org/wiki/Stable_Diffusion) using a basic dataset like Flickr30k\n",
    "- Display some samples of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "installs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: IMPORTS\n",
    "- We need `torch`, `numpy`, `math`, and `matplotlib` as our usual ML imports\n",
    "- We also need `einops` for tensor rearranging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import functools\n",
    "import math\n",
    "from einops import rearrange\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import MultiplicativeLR, LambdaLR\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: DATASET\n",
    "- We will be using the [Flickr30kDataset](https://paperswithcode.com/dataset/flickr30k) from our [last CLIP project](https://github.com/samherring99/NightwingCurriculum/blob/main/module_4_diffusion_models/module_4_project_1.ipynb) to make things simple\n",
    "- We include a transform to resize the images to 112x112px (to reduce memory usage) and convert them to a tensor\n",
    "- We set 2 possible captions per image, and a batch size of 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flickr30kDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.dataset = load_dataset(\"nlphuji/flickr30k\", cache_dir=\"./huggingface_data\")\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((112, 112)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        self.cap_per_image = 2\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_rows[\"test\"] * self.cap_per_image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        original_idx = idx // self.cap_per_image\n",
    "        image = self.dataset[\"test\"][original_idx][\"image\"].convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        caption = self.dataset[\"test\"][original_idx][\"caption\"][idx % self.cap_per_image]\n",
    "\n",
    "        return {\"image\": image, \"caption\": caption}\n",
    "    \n",
    "flickr30k_dataset = Flickr30kDataset()\n",
    "print(len(flickr30k_dataset))\n",
    "flickr_dataloader = DataLoader(flickr30k_dataset, batch_size=128, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: TOKENIZATION\n",
    "- We will reuse our simple tokenization from [this GPT project](https://github.com/samherring99/NightwingCurriculum/blob/main/module_2_advanced_nlp_and_transformers/module_2_project_1.ipynb)\n",
    "- This makes things simpler so we can focus on the architecture and design of SD\n",
    "- We provide the usual `encode` and `decode` methods to tokenize our captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = ''.join([str(batch['caption']) for batch in flickr_dataloader])\n",
    "\n",
    "chars = sorted(list(set(captions)))\n",
    "\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: PROJECTION LAYER\n",
    "- We want to create a Projection layer that performs a [GaussianFourierProjection](https://mathworld.wolfram.com/FourierTransformGaussian.html)\n",
    "- Our objective here depends on time (noise added to/removed from an image over `t` time steps), so we need to emded time variations throughout our model's training\n",
    "- To do this, we take a random sample of the weights at initialization that remain unchanged\n",
    "- We then calculate the cosine and sine projections of variations in the time step tensors against the original weights of the time steps\n",
    "- This offers a Gaussian random features we can use to capture temporal patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, scale=30.):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_proj = x[:, None] * self.W[None, :] * 2 * np.pi\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: FULLY CONNECTED LAYER\n",
    "- This is a simple fully connected layer that has one Linear projection from `input_dim` to `output_dim`\n",
    "- The forward pass adds two dimensions to the output tensor to be of shape [B, C, H, W] wher B is the batch dimension, C is the number of channels, and H&W are the height and width of the image respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dense(x)[..., None, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: SAMPLER\n",
    "- Here we are building our sampler, which samples from score-based models using the [Euler-Maruyama solver](https://en.wikipedia.org/wiki/Euler%E2%80%93Maruyama_method)\n",
    "- The first method, `marginal_prob_std`, caclulates the mean and standard deviation of an exponentially increasing noise level, given our time step vector and a `sigma` parameter, which is our initial noise level (25.0 here)\n",
    "- The second method uses the same input parameters to compute the [diffusion coefficients](https://en.wikipedia.org/wiki/Diffusion_equation) used in sampling by multiplying the diffusion coefficients over the vector of time steps\n",
    "- Lastly, we can create our partial functions and our sampler method\n",
    "- We initialize a time step tensor of size `batch_size` and calulate `init_x` which is our starting image tensor\n",
    "- We set our `time_steps` and `step_size` to be a 1D tensor of 500 steps of equal size from 1.0 to 0.001\n",
    "- We then iterate over our time steps, calculate our diffusion coefficient `g` and apply it, followed by our scoring model to calulate the mean denoising intermediary image\n",
    "- Lastly, we update our initial image tensor by adding the intermediary denoising image tensor, and return the denoising tensor at the last timestep\n",
    "- By including `y` in our scoring model we can pass in text as a 'caption' and the model will generate the closest possible image representation to that given text\n",
    "- This method generates samples based on provided text using our score-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "def marginal_prob_std(t, sigma):\n",
    "    t = torch.tensor(t, device=device)\n",
    "    return torch.sqrt((sigma**(2 * t) - 1.) / 2. / np.log(sigma))\n",
    "\n",
    "def diffusion_coeff(t, sigma):\n",
    "    return torch.tensor(sigma**t, device=device)\n",
    "\n",
    "sigma =  25.0\n",
    "marginal_prob_std_fn = functools.partial(marginal_prob_std, sigma=sigma)\n",
    "diffusion_coeff_fn = functools.partial(diffusion_coeff, sigma=sigma)\n",
    "\n",
    "def sampler(score_model,\n",
    "                           marginal_prob_std,\n",
    "                           diffusion_coeff,\n",
    "                           batch_size=64,\n",
    "                           x_shape=(3, 112, 112),\n",
    "                           num_steps=500,\n",
    "                           device='cuda',\n",
    "                           eps=1e-3, y=None):\n",
    "\n",
    "    t = torch.ones(batch_size, device=device)\n",
    "    init_x = torch.randn(batch_size, *x_shape, device=device) * marginal_prob_std(t)[:, None, None, None]\n",
    "    \n",
    "    time_steps = torch.linspace(1., eps, num_steps, device=device)\n",
    "    step_size = time_steps[0] - time_steps[1]\n",
    "    x = init_x\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for time_step in range(time_steps):\n",
    "            batch_time_step = torch.ones(batch_size, device=device) * time_step\n",
    "            g = diffusion_coeff(batch_time_step)\n",
    "            mean_x = x + (g**2)[:, None, None, None] * score_model(x, batch_time_step, y=y) * step_size\n",
    "            x = mean_x + torch.sqrt(step_size) * g[:, None, None, None] * torch.randn_like(x)\n",
    "    \n",
    "    return mean_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7: ATTENTION\n",
    "- Now we can start building our pieces for our Transformer blocks\n",
    "- We will use attention the same way as in previous projects\n",
    "- This implementation is just single-headed attention for simplicity\n",
    "- We project the query to a linear layer, and assign our keys and values based on cross vs self attention\n",
    "- After the forward pass, we use `torch.einsum` to perform the inner product of the query and key tensors \n",
    "- We then take the softmax and multiply this by the value tensor, the result of which will be returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, context_dim=None, num_heads=1):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.context_dim = context_dim\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.query = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
    "        \n",
    "        if context_dim is None:\n",
    "            self.self_attn = True\n",
    "            self.key = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
    "            self.value = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        else:\n",
    "            self.self_attn = False\n",
    "            self.key = nn.Linear(context_dim, embed_dim, bias=False)\n",
    "            self.value = nn.Linear(context_dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, tokens, context=None):\n",
    "        if self.self_attn:\n",
    "            Q = self.query(tokens)\n",
    "            K = self.key(tokens)\n",
    "            V = self.value(tokens)\n",
    "        else:\n",
    "            Q = self.query(tokens)\n",
    "            K = self.key(context)\n",
    "            V = self.value(context)\n",
    "\n",
    "        new_K = torch.squeeze(K)\n",
    "        new_V = torch.squeeze(V)\n",
    "\n",
    "        scoremats = torch.einsum(\"BTH,BSH->BTS\", Q, new_K)\n",
    "        attnmats = F.softmax(scoremats / math.sqrt(self.embed_dim), dim=-1)\n",
    "        ctx_vecs = torch.einsum(\"BTS,BSH->BTH\", attnmats, new_V)\n",
    "\n",
    "        return ctx_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8: BASIC TRANSFORMER\n",
    "- Here we implement a basic Transformer block with both self and cross attention, one feed forward network, and 3 layernorms\n",
    "- We perform self attention after normalizing, then add the residual connection\n",
    "- Next, we perform cross attention with the provided caption (self attention again if none is provided) and add residual again\n",
    "- Lastly, we run our feed forward layer with GeLU activation and add the final residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, hidden_dim, context_dim):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.attn_self = Attention(hidden_dim, hidden_dim)\n",
    "        self.attn_cross = Attention(hidden_dim, hidden_dim, context_dim)\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm3 = nn.LayerNorm(hidden_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 3 * hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(3 * hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "        x = self.attn_self(self.norm1(x)) + x\n",
    "        x = self.attn_cross(self.norm2(x), context=context) + x\n",
    "        x = self.ffn(self.norm3(x)) + x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 9: IMAGE TRANSFORMER\n",
    "- Using our `Transformer` class from above, we can build a spatial Transformer\n",
    "- This simply helps shape the data using `einops` so the Transformer can take in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransformer(nn.Module):\n",
    "    def __init__(self, hidden_dim, context_dim):\n",
    "        super(ImageTransformer, self).__init__()\n",
    "        self.transformer = Transformer(hidden_dim, context_dim)\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "        b, c, h, w = x.shape\n",
    "        x_in = x\n",
    "        x = rearrange(x, \"b c h w -> b (h w) c\")\n",
    "        x = self.transformer(x, context)\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "        return x + x_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 10: UNET TRANSFORMER ARCHITECTURE\n",
    "- We can finally build out UNet that will provide the upscaling/downscaling needed for training\n",
    "- Our \n",
    "- We create our time embedding projection layer followed by a linear projection layer\n",
    "- We then have 2 down-convolution layers that follow: [Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html), fully connected layer, then `nn.GroupNorm`\n",
    "- This takes the image from size 112x112 to size 27x27\n",
    "- The next 2 down-convolution layers are the same but have a Spatial Transformer inserted after the normalization to compute attention\n",
    "- Next, we have 3 up-convolution layers that follow: [ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html), fully connected layer, then `nn.GroupNorm` followed by one more ConvTranspose2d\n",
    "- Lastly, we define our activation function and set the `marginal_prob_std` method\n",
    "- For the forward pass, we embed our timestep vector and the caption\n",
    "- We then go down the encoding path (down-scaling), applying convolution, fully connected layer, nromalization, and the activation function (with attention for the last two steps)\n",
    "- Next, we go through our encoding path, applying the same steps as above but up-scaling and normalizing at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, marginal_prob_std, channels=[32, 64, 128, 256], embed_dim=256,\n",
    "                 text_dim=256, nClass=len(chars)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.time_embed = nn.Sequential(\n",
    "            ProjectionLayer(embed_dim=embed_dim),\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, channels[0], 3, stride=1, bias=False)\n",
    "        self.dense1 = FCLayer(embed_dim, channels[0])\n",
    "        self.gnorm1 = nn.GroupNorm(4, num_channels=channels[0])\n",
    "\n",
    "        self.conv2 = nn.Conv2d(channels[0], channels[1], 3, stride=2, bias=False)\n",
    "        self.dense2 = FCLayer(embed_dim, channels[1])\n",
    "        self.gnorm2 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "\n",
    "        self.conv3 = nn.Conv2d(channels[1], channels[2], 3, stride=2, bias=False)\n",
    "        self.dense3 = FCLayer(embed_dim, channels[2])\n",
    "        self.gnorm3 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "        self.attn3 = ImageTransformer(channels[2], text_dim)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(channels[2], channels[3], 3, stride=2, bias=False)\n",
    "        self.dense4 = FCLayer(embed_dim, channels[3])\n",
    "        self.gnorm4 = nn.GroupNorm(32, num_channels=channels[3])\n",
    "        self.attn4 = ImageTransformer(channels[3], text_dim)\n",
    "\n",
    "        self.tconv4 = nn.ConvTranspose2d(channels[3], channels[2], 3, stride=2, bias=False)\n",
    "        self.dense5 = FCLayer(embed_dim, channels[2])\n",
    "        self.tgnorm4 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "\n",
    "        self.tconv3 = nn.ConvTranspose2d(channels[2], channels[1], 3, stride=2, bias=False, output_padding=1)\n",
    "        self.dense6 = FCLayer(embed_dim, channels[1])\n",
    "        self.tgnorm3 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "\n",
    "        self.tconv2 = nn.ConvTranspose2d(channels[1], channels[0], 3, stride=2, bias=False, output_padding=1)\n",
    "        self.dense7 = FCLayer(embed_dim, channels[0])\n",
    "        self.tgnorm2 = nn.GroupNorm(32, num_channels=channels[0])\n",
    "        self.tconv1 = nn.ConvTranspose2d(channels[0], 1, 3, stride=1)\n",
    "\n",
    "        self.act = nn.SiLU()\n",
    "        self.marginal_prob_std = marginal_prob_std\n",
    "        self.cond_embed = nn.Embedding(nClass, text_dim)\n",
    "\n",
    "    def forward(self, x, t, y=None):\n",
    "        embed = self.act(self.time_embed(t))\n",
    "        y = y.long()\n",
    "        y_embed = self.cond_embed(y).unsqueeze(1)\n",
    "\n",
    "        h1 = self.conv1(x) + self.dense1(embed)\n",
    "        h1 = self.act(self.gnorm1(h1))\n",
    "        h2 = self.conv2(h1) + self.dense2(embed)\n",
    "        h2 = self.act(self.gnorm2(h2))\n",
    "        h3 = self.conv3(h2) + self.dense3(embed)\n",
    "        h3 = self.act(self.gnorm3(h3))\n",
    "        h3 = self.attn3(h3, y_embed)\n",
    "        h4 = self.conv4(h3) + self.dense4(embed)\n",
    "        h4 = self.act(self.gnorm4(h4))\n",
    "        h4 = self.attn4(h4, y_embed)\n",
    "\n",
    "        h = self.tconv4(h4) + self.dense5(embed)\n",
    "        h = self.act(self.tgnorm4(h))\n",
    "        h3_resized = F.interpolate(h3, size=(h.size(2), h.size(3)), mode='bilinear', align_corners=True)\n",
    "        h = self.tconv3(h + h3_resized) + self.dense6(embed)\n",
    "        h = self.act(self.tgnorm3(h))\n",
    "        h2_resized = F.interpolate(h2, size=(h.size(2), h.size(3)), mode='bilinear', align_corners=True)\n",
    "        h = self.tconv2(h + h2_resized) + self.dense7(embed)\n",
    "        h = self.act(self.tgnorm2(h))\n",
    "        h1_resized = F.interpolate(h1, size=(h.size(2), h.size(3)), mode='bilinear', align_corners=True)\n",
    "        h = self.tconv1(h + h1_resized)\n",
    "\n",
    "        h_resized = F.interpolate(h, size=(112, 112), mode='bilinear', align_corners=True)\n",
    "\n",
    "        h = h_resized / self.marginal_prob_std(t)[:, None, None, None]\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 11: LOSS FUNCTION\n",
    "- We need to implement our loss function for our model\n",
    "- This function takes in the `x` and `y` components of the batch (the image and caption)\n",
    "- Then, randomly sample our timesteps and our random noise\n",
    "- Use our `marginal_prob_std` method with our random time sample to get standard deviation of perturbation kernel at `t`\n",
    "- Perturb the input data `x` using our noise `z` and our standard deviation\n",
    "- Score the model and use the score to calculate loss, return it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, x, y, marginal_prob_std, eps=1e-5):\n",
    "    random_t = torch.rand(x.shape[0], device=x.device) * (1. - eps) + eps\n",
    "    z = torch.randn_like(x)\n",
    "    std = marginal_prob_std(random_t)\n",
    "    perturbed_x = x + z * std[:, None, None, None]\n",
    "    score = model(perturbed_x, random_t, y=y)\n",
    "    loss = torch.mean(torch.sum((score * std[:, None, None, None] + z)**2, dim=(1, 2, 3)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 12: HYPERPARAMETERS\n",
    "- First, we set up our score model to be out `UNet` from above\n",
    "- We then set 100 epochs with a learning rate of 0.001\n",
    "- We will be using a basic Adam optimizer with default parameters for simplicity and a LambdaLR scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_model = torch.nn.DataParallel(UNet(marginal_prob_std=marginal_prob_std_fn))\n",
    "score_model = score_model.to(device)\n",
    "\n",
    "n_epochs = 100\n",
    "lr = 10e-4         \n",
    "\n",
    "optimizer = Adam(score_model.parameters(), lr=lr)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: max(0.2, 0.98 ** epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 13: TRAINING LOOP\n",
    "- Now we can set up our training loop for our diffusion model\n",
    "- We iterate over our epochs, for each one we iterate through our dataloader\n",
    "- For each batch in the dataloader, we load the images into `x` and the captions into `y`\n",
    "- We then perturb and compute the loss using our loss function\n",
    "- After, we step our optimizer and after all batches in the epoch, we step the scheduler\n",
    "- Finally, once the epochs finish we save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs = range(n_epochs)\n",
    "for epoch in total_epochs:\n",
    "    avg_loss = 0.\n",
    "    num_items = 0\n",
    "\n",
    "    for batch in flickr_dataloader:\n",
    "        x = batch['image'].to(device)\n",
    "\n",
    "        caption = []\n",
    "\n",
    "        for string in batch['caption']:\n",
    "            caption.append(torch.Tensor(encode(str(string))).to(device))\n",
    "\n",
    "        y = pad_sequence(caption, batch_first=True)\n",
    "\n",
    "        loss = loss_fn(score_model, x, y, marginal_prob_std_fn)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item() * x.shape[0]\n",
    "        num_items += x.shape[0]\n",
    "\n",
    "    scheduler.step()\n",
    "    lr_current = scheduler.get_last_lr()[0]\n",
    "\n",
    "    print('{} Average Loss: {:5f} lr {:.1e}'.format(epoch, avg_loss / num_items, lr_current))\n",
    "\n",
    "    torch.save(score_model.state_dict(), 'ckpt_transformer.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 14: SAMPLE\n",
    "- Here we are going to display some generated images by passing in a string prompt and sampling images\n",
    "- We will use our `sampler` created above for this, with 500 steps and a batch size of 1\n",
    "- We initialize our sampler with our standard deviation and diffusion coefficient methods created above\n",
    "- And we then add our prompt into `y`\n",
    "- After sample generation, we clamp the samples to range [0.0, 1.0] and display/save the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch_size = 1\n",
    "num_steps = 500\n",
    "\n",
    "sample = sampler(score_model,\n",
    "                  marginal_prob_std_fn,\n",
    "                  diffusion_coeff_fn,\n",
    "                  sample_batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  device=device,\n",
    "                  y=torch.Tensor(encode(\"a tuxedo cat sitting on a couch\")))\n",
    "\n",
    "sample = sample.clamp(0.0, 1.0)\n",
    "\n",
    "img = sample[0].permute(1, 2, 0)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(img.cpu(), vmin=0., vmax=1.)\n",
    "plt.savefig('sample_image.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
